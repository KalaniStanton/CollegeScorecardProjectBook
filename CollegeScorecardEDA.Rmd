--- 
title: "An Exploratory Analysis of Data in the U.S. College Scorecard"
author: "Sara Haman, Adam Lashley, Reilly Stanton, Benjamin Weisman"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
output_dir: "docs"
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes~
github-repo:  KalaniStanton/CollegeScorecardProjectBook 
description: "An analysis of data from the College Scorecard API, which focuses on differences identified between institutions with R1 and R2 carnegie classifications."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Preface {-}


This project was completed to satisfy the requirements of a project assigned to students in the Fall 2020 Data Munging and Exploratory Data Analysis at New College of Florida.

This document is broken down into sections pertaining to the discrete steps in the process of exploratory data analysis. 

In the first chapter (\@ref(intro)), we present an overview of this project. In the second chapter (\@ref(import)), we outline how we accessed and cleaned the data. We demonstrate our initial attempts at pulling the data directly from the API, and explain why we chose to pivot to using the {rscorecard} package for querying our data from the API. In the third (\@ref(quality)), we compute scores on each of the data quality metrics described in Pipino, Lee, and Wang (2002). Although we handle NA’s while cleaning the data, the data quality metrics are performed on the initial data pulls, so that our assessment procedure is generally reproducible. The fourth chapter (\@ref(clean)) provides the code used to clean the data. Then, we begin the analysis of the data (\@ref(analysis)). This is done in two steps: first, by exploring the descriptive variables in order to glean the composition of the data set, and second by investigating our theory that the proportion of White students at a school will positively influence student outcomes (possibly due to implicit favor given to White individuals). We examine features that R1 and R2 schools differ on, and provide a model for the relationship between demographics and student outcomes. The results of these tests are reported (\@ref(analysis)). In this discussion (\@ref(discussion)), we summarize the key findings from the analysis. 


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# Introduction {#intro}


The Carnegie Classification system classifies American universities by their type and their research productivity. Type is determined by the range of degrees a university awards. Institutions which award at least 20 research-focused doctoral degrees a year are categorized as doctoral universities. Within doctoral universities, schools are given secondary ratings commensurate with the extent of their research expenditures. These ratings, which include R1, R2, and D/PU,  are colloquially referenced as a measure of prestige. R1 schools, or schools with “very high research activity”, are typically represented by old and infamous universities, such as Harvard, and flagship state institutions. However, an R1 designation is not indicative of the quality of research produced, nor the caliber of education one receives at the school. The two factors which influence a school’s “R” status are the ratio of doctoral degrees awarded over faculty count, and the monetary amount the school invests into research. This is to say, an institution could conduct horrible, hackneyed research that never gets published except in predatory journals and still earn an R1 designation if they spend at least 5 million dollars on research expenditures (The Carnegie Classification of Institutes of Higher Education, 2018). Of course, this situation is not realistically feasible. Donors would abandon a school producing blatantly unproductive research. Yet, the reputation of a school is, at some level, intertwined with their “R” designation. When Dartmouth’s designation was decremented in 2016, media outlets reported on it as if it were shameful (Anderson, 2016). 

Without context for how Carnegie Classifications are assigned, the R1 designation may seem “superior” to R2. Our group chose to investigate the influence of the Carnegie Classification of an institution on student outcomes; essentially, if attending an R1 school provides a tangible benefit over attending an R2. Furthermore, we explore features of R1 and R2 universities to determine which variables they significantly differ on.


<!--chapter:end:01-intro.Rmd-->

# Data Acquisition {#import}

```{r, message = FALSE}
library(plyr)
library(tidyverse)
library(Hmisc)
library(httr)
library(jsonlite)
library(tidyverse)
#install.packages("tmap")
library(tmap)
library(leaflet)
library(treemap)
library(kableExtra)

`%notin%` <- Negate(`%in%`)

MakePretty <- function(x) {
  x %>% kbl(align = "c") %>% kable_material()
}
```

## Set-up and API Exploration

For our project, we wanted to practice using APIs to pull the data into our global environment and extract our desired variables using base R.

As you will see, our attempt failed, but was useful in helping us understand the relational structure of the College Scorecard Data and the limits of simple indexing when dealing with complicated data structures. 

### Project Directory Management

Prior to extracting any of the data, we want to ensure that our project is reproducible. Thus, we first create a sub-directory within the user's working directory, which will hold the .csv files we will write containing our variables of interest.

```{r dir-setup}
proj.dir <- getwd()
#dir.create("Proj1Data")
data.dir <- paste0(proj.dir, "/Proj1Data")
```

Next, we pass the API key into an object so that it can be called in the `GET()` function, which pulls the data from the API into out global environment.

```{r api-setup}
APIKey = "FELgrGb47PaevTWxqZTt6etFaQVnDbKpcJLaPL6a"
res = GET(paste0("https://api.data.gov/ed/collegescorecard/v1/schools?api_key=", APIKey))
```

This data is initially in a `raw` format that is pulled from a JSON file stored on the API servers. To do this, we first convert the `raw` data to `char` and call the `fromJSON()` function to convert the data to a more "R friendly" data structure.

```{r data-setup}
data = fromJSON(rawToChar(res$content))
is(data)
```
Now that this raw JSON file is converted to a list, we can begin our exploration of the data.

### API Exploration

We originally intended on using `list2env(yearsLi, envir = .GlobalEnv)` to split the list into multiple `data.frame` objects, but the fact that the names of the listed data frames are numeric values will be a problem. When calling these data frames, R will have to decide whether the user input `r 2012` is calling the `data$2012` data set, or the number `2012`. This is likely to cause problems in our later analysis, so we have to look at the `names()` of objects in our `data` list to see how we might adjust our approach to exploring the data. 

```{r data-names}
names(data$results)
```

Upon looking at the `names()` we noticed that not all of the listed data frames are named after years, and thus are likely contain different data than the others. 

Therefore, we first extract the non-year data frames, prior to extracting the year data frames with a character alteration (to make them non-numeric), 

```{r data-subset}
id_data <- data$results[c("school", "id", "location", "ope6_id", "ope8_id")]

yearsLi<- data$results[names(data$results) %notin% c("school", "id", "location", "ope6_id", "ope8_id")]

names(yearsLi) <- paste0("yr", names(yearsLi))

#list2env(yearsLi, envir = .GlobalEnv)
```

At this point, we then called `list2env(data$results, envir = .GlobalEnv)` to create individual dataframes of each object in the `yearsLi` list, which helped us understand what was happening at different levels of the data structure, but ultimately left us more confused about how we should approach querying this data. 

This code, as well as the code we used to analyze these objects below, is commented it out because it creates an unnecessary number of objects in the global environment, but retained in this document to show the attempts we made at exploring the data structure.
```{r}
#
#names(yr2012)
#contents(id_data$school)
#contents(yr2012$academics$program$bachelors)
#yr2012$academics$program$bachelors
#length(id_data$ope8_id)
```

At this point, the challenge of collecting data directly from the API seemed insurmountable, but we sought a solution and turned to the sage of all data science obstacles: Google.

## The `rscorecard` and `tidyverse` approach

As expected, we found out savior in the annals of algorithmic wisdom when we stumbled upon a package called [{rscorecard}](https://www.btskinner.io/rscorecard/index.html).

The [{rscorecard}](https://www.btskinner.io/rscorecard/index.html) package is a wrapper for the College Scorecard API that takes full advantage of the complexity in this relational data structure by employing useful tools from the [tidyverse](https://www.tidyverse.org/) eco-system. Using [{dplyr}](https://dplyr.tidyverse.org/) like functions, specified to this data set, and pipe operators `%>%` from [{magrittr}](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html), the rscorecard package provides an astonishingly simple solution for querying data directly from the API.

```{r}
#install.packages("rscorecard")
library(rscorecard)
```

To access the API in this package we pass the object holding our API Key [as a string] into the `sc_key()` function.

```{r}
sc_key(APIKey)
```
As the message above states, once the API key is set, the user can append conditions to the query function `sc_get()` to extract the data they desire. These conditions are applied using functions in the package, which are appended to the query function using the aforementioned pipes. The primary functions for conditioning the queries are `sc_filter()`, to collect observations that satisfy boolean expressions, `sc_select()`, to subset the data by columns, and `sc_year()`, to select the data from a specified year.

These functions are what we use below to extract data on **Finances**, **Demographics**, and **Death Rates** from schools that classify as **R1** and **R2** research institutions.

### Finances

```{r}
F_latest <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr, npt4_pub, npt4_priv, costt4_a, grad_debt_mdn, avgfacsal, unemp_rate) %>% 
    sc_year("latest") %>% 
    sc_get()

F_2018 <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr, npt4_pub, npt4_priv, costt4_a, grad_debt_mdn, avgfacsal, unemp_rate) %>% 
    sc_year(2018) %>% 
    sc_get()

F_2017 <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr, npt4_pub, npt4_priv, costt4_a, grad_debt_mdn, avgfacsal, unemp_rate) %>%
    sc_year(2017) %>% 
    sc_get()

F_2016 <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr, npt4_pub, npt4_priv, costt4_a, grad_debt_mdn, avgfacsal, unemp_rate) %>% 
    sc_year(2016) %>% 
    sc_get()

F_2015 <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr, npt4_pub, npt4_priv, costt4_a, grad_debt_mdn, avgfacsal, unemp_rate) %>% 
    sc_year(2015) %>% 
    sc_get()
```

```{r, results = 'hide'}
F_latest %>% mutate(year = "latest")
F_2018 %>% mutate(year = 2018)
F_2017 %>% mutate(year = 2017)
F_2016 %>% mutate(year = 2016)
F_2015 %>% mutate(year = 2015)

Finance <- rbind(F_latest, F_2018, F_2017, F_2016, F_2015)
```

```{r}
write.csv(Finance, "Proj1Data/Financials15to19.csv")
```

### Demographics

```{r}
D_latest <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr,female, first_gen, poverty_rate, veteran, unemp_rate, ugds_white, ugds_black, ugds_hisp, ugds_asian, ugds_aian, ugds_nhpi, ugds_2mor, ugds_nra, ugds_unkn, ugds_whitenh, ugds_blacknh, ugds_api, ugds_aianold, ugds_hispold) %>% 
    sc_year("latest") %>% 
    sc_get()

D_2018 <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr, female, first_gen, poverty_rate, veteran, unemp_rate, ugds_white, ugds_black, ugds_hisp, ugds_asian, ugds_aian, ugds_nhpi, ugds_2mor, ugds_nra, ugds_unkn, ugds_whitenh, ugds_blacknh, ugds_api, ugds_aianold, ugds_hispold) %>% 
    sc_year(2018) %>% 
    sc_get()

D_2017 <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr, female, first_gen, poverty_rate, veteran, unemp_rate, ugds_white, ugds_black, ugds_hisp, ugds_asian, ugds_aian, ugds_nhpi, ugds_2mor, ugds_nra, ugds_unkn, ugds_whitenh, ugds_blacknh, ugds_api, ugds_aianold, ugds_hispold) %>%
    sc_year(2017) %>% 
    sc_get()

D_2016 <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr, female, first_gen, poverty_rate, veteran, unemp_rate, ugds_white, ugds_black, ugds_hisp, ugds_asian, ugds_aian, ugds_nhpi, ugds_2mor, ugds_nra, ugds_unkn, ugds_whitenh, ugds_blacknh, ugds_api, ugds_aianold, ugds_hispold) %>% 
    sc_year(2016) %>% 
    sc_get()

D_2015 <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr, female, first_gen, poverty_rate, veteran, unemp_rate, ugds_white, ugds_black, ugds_hisp, ugds_asian, ugds_aian, ugds_nhpi, ugds_2mor, ugds_nra, ugds_unkn, ugds_whitenh, ugds_blacknh, ugds_api, ugds_aianold, ugds_hispold) %>% 
    sc_year(2015) %>% 
    sc_get()
```

```{r, results = 'hide'}
D_latest %>% mutate(year = "latest")
D_2018 %>% mutate(year = 2018)
D_2017 %>% mutate(year = 2017)
D_2016 %>% mutate(year = 2016)
D_2015 %>% mutate(year = 2015)

Demographics <- rbind(D_latest, D_2018, D_2017, D_2016, D_2015)
```

```{r}
write.csv(Demographics, "Proj1Data/Demographics15to19.csv")
```

### Death

```{r}
Dth_latest <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr, death_yr2_rt, lo_inc_death_yr2_rt, md_inc_death_yr2_rt,  hi_inc_death_yr2_rt, death_yr3_rt, lo_inc_death_yr3_rt, md_inc_death_yr3_rt, hi_inc_death_yr3_rt, death_yr4_rt, lo_inc_death_yr4_rt, md_inc_death_yr4_rt, hi_inc_death_yr4_rt, death_yr6_rt, lo_inc_death_yr6_rt, md_inc_death_yr6_rt, hi_inc_death_yr6_rt, death_yr8_rt, lo_inc_death_yr8_rt, md_inc_death_yr8_rt, hi_inc_death_yr8_rt) %>% 
    sc_year("latest") %>% 
    sc_get()

Dth_2018 <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr,death_yr2_rt, lo_inc_death_yr2_rt, md_inc_death_yr2_rt,  hi_inc_death_yr2_rt, death_yr3_rt, lo_inc_death_yr3_rt, md_inc_death_yr3_rt, hi_inc_death_yr3_rt, death_yr4_rt, lo_inc_death_yr4_rt, md_inc_death_yr4_rt, hi_inc_death_yr4_rt, death_yr6_rt, lo_inc_death_yr6_rt, md_inc_death_yr6_rt, hi_inc_death_yr6_rt, death_yr8_rt, lo_inc_death_yr8_rt, md_inc_death_yr8_rt, hi_inc_death_yr8_rt) %>% 
    sc_year(2018) %>% 
    sc_get()

Dth_2017 <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr, death_yr2_rt, lo_inc_death_yr2_rt, md_inc_death_yr2_rt,  hi_inc_death_yr2_rt, death_yr3_rt, lo_inc_death_yr3_rt, md_inc_death_yr3_rt, hi_inc_death_yr3_rt, death_yr4_rt, lo_inc_death_yr4_rt, md_inc_death_yr4_rt, hi_inc_death_yr4_rt, death_yr6_rt, lo_inc_death_yr6_rt, md_inc_death_yr6_rt, hi_inc_death_yr6_rt, death_yr8_rt, lo_inc_death_yr8_rt, md_inc_death_yr8_rt, hi_inc_death_yr8_rt) %>%
    sc_year(2017) %>% 
    sc_get()

Dth_2016 <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr, death_yr2_rt, lo_inc_death_yr2_rt, md_inc_death_yr2_rt,  hi_inc_death_yr2_rt, death_yr3_rt, lo_inc_death_yr3_rt, md_inc_death_yr3_rt, hi_inc_death_yr3_rt, death_yr4_rt, lo_inc_death_yr4_rt, md_inc_death_yr4_rt, hi_inc_death_yr4_rt, death_yr6_rt, lo_inc_death_yr6_rt, md_inc_death_yr6_rt, hi_inc_death_yr6_rt, death_yr8_rt, lo_inc_death_yr8_rt, md_inc_death_yr8_rt, hi_inc_death_yr8_rt) %>% 
    sc_year(2016) %>% 
    sc_get()

Dth_2015 <- sc_init() %>%
    sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %>%
    sc_select(control, instnm, ccbasic, stabbr, death_yr2_rt, lo_inc_death_yr2_rt, md_inc_death_yr2_rt,  hi_inc_death_yr2_rt, death_yr3_rt, lo_inc_death_yr3_rt, md_inc_death_yr3_rt, hi_inc_death_yr3_rt, death_yr4_rt, lo_inc_death_yr4_rt, md_inc_death_yr4_rt, hi_inc_death_yr4_rt, death_yr6_rt, lo_inc_death_yr6_rt, md_inc_death_yr6_rt, hi_inc_death_yr6_rt, death_yr8_rt, lo_inc_death_yr8_rt, md_inc_death_yr8_rt, hi_inc_death_yr8_rt) %>% 
    sc_year(2015) %>% 
    sc_get()
```

```{r, results = 'hide'}
Dth_latest %>% mutate(year = "latest")
Dth_2018 %>% mutate(year = 2018)
Dth_2017 %>% mutate(year = 2017)
Dth_2016 %>% mutate(year = 2016)
Dth_2015 %>% mutate(year = 2015)

Death <- rbind(Dth_latest, Dth_2018, Dth_2017, Dth_2016, Dth_2015)
```

```{r}
write.csv(Death, "Proj1Data/Death15to19.csv")
```


```{r}
R1Schools <- sc_init() %>%
    sc_filter(ccbasic == 15) %>%
    sc_select(control, instnm, stabbr) %>% 
    sc_year("latest") %>% 
    sc_get()
```


<!--chapter:end:02-DataAcquisition.Rmd-->

# Data Quality {#quality}

## Set-up

First, I import the packages that I use in this section and read in the uncleaned data.

```{r, message = FALSE}
#LIBRARIES 
library(tidyverse)
library(kableExtra)
MakePretty <- function(x) {
  x %>% kbl(align = "c") %>% kable_material()
}
```

```{r, message = FALSE}

#Reading in the data
fin <- read.csv('Proj1Data/Financials15to19.csv')
fin <- fin[,-1]

dem <- read.csv("Proj1Data/Demographics15to19.csv")
dem <- dem[,-1]

```

## The Functions 

**CONSISTANT REPRESENTATION**

```{r}

con_rep <- function(df){
  "
  A function that quantitatively scores an input data frame on the consistancy of representation data quality metric.
  
  Input: 
    df: a data frame
  Output: 
    con_rep_score: A numeric score on consistency of representation ranging from 1 to 0, where 1 is perfectly consistent representation and 0 is inconsistent representation.
  "
  
  type = vector()
  for(i in 1:ncol(df)){
    col_type <- typeof(df[1,i])
    type[i] <- col_type
  }
  
  con_rep_score <- 1 - ((length(unique(type)) - 1)/6)
  return(con_rep_score)
}

```

**COMPLETENESS AND EASE OF MANIPULATION**

```{r}

data_quality <- function(df){
  
  "
  A function to quantitatively compute scores for a dataframe on the completeness and ease of manipulation data quality metrics. 
  
  Input: 
    df: A data frame
    
  Output: 
    qualityTable: A table reporting the scores on completeness and ease of manipulation for each column in the input data frame. 
  "
  
  # Setting the index value, which will be used to index the column name 

  index <- 1
  
  # Instantiating empty data frames for each of the queries
  
  completeness <- data.frame(Completeness=double())
  eom <- data.frame(Ease_of_Manipulation=double())
  names <- data.frame(ColumnName=character())
  
  # Populating the data frames using a for-loop
  
  for (i in df){
    
    # COLLECTING THE NAMES OF EACH COLUMN PASSED
    
    col <- colnames(df[index])
    
    # COMPLETENESS
    # Takes the sum of the total NA, NULL, and NaN values in a column
    # Divides them by the length of the column
    # Subtracts this from one, as was suggested by Pipinio, Lee, and Wang
    # And then rounds to output to the third decimal place
    
    c <- 1-(sum(is.na(i) + is.null(i) + is.nan(i))/length(i)) %>%
      round(digits = 3)
    
    # EASE OF MANIPULATION
    # "Case when" vectorises a series of if/else statements
    # The function checks the type of the column and then sets the variable,
    # e, to the corresponding value. 
    
    e <- case_when(
      typeof(i) == "logical" ~ 1,
      typeof(i) == "integer" ~ .9,
      typeof(i) == "numeric" ~ .8,
      typeof(i) == "double" ~ .8,
      typeof(i) == "complex" ~ .7,
      typeof(i) == "character" ~ .6,
      typeof(i) == "list" ~ .5, 
      typeof(i) == "raw" ~ 0,
      TRUE ~ 0)
    
    #The index used to collect column names is increased by one
    
    index = index + 1
    
    #Appending the output for each column to their respective data frames
    
    completeness[nrow(completeness)+1,] <- c
    eom[nrow(eom)+1,] <- e
    names[nrow(names)+1,] <- col
  }
  
  #Binding the columns of the three tables into an output table
  qualityTable <- cbind(names, completeness, eom)
  
  return(qualityTable)
}

```


## The Evaluations 

We assessed data quality using the metrics outlined in (paper). For each of these metrics, we provide a brief commentary on how the data fared. Several of the data quality metrics were more pertinent to our analysis, so we provide deeper insight into them.

**Accessibility**
On its own, the data were not accessible and extraction proved to be exceedingly challenging. By using the {rscorecard} package, we were able to streamline the process. However, {rscorecard} is not built into the API and cannot be factored into the dataset's baseline accessibility. Despite the challenge of pulling the data into an interpretable form, they can be accessed by the public. This is to say, we did not have to fight with a Facebook executive to access them.  We scored the data at a 4/10 for accessibility.

**Believability**
There is no reason for us to assume the data would be intentionally falsified as it was collected by the U.S. Government to simply compare costs and values of higher education institutions. There have been claims from some colleges (such as Boston University) that there are inaccuracies in the data, however. We decided to give the data a score of 8/10 on believability. 

**Concise Representation**
The data is difficult to navigate through without frequent cross-checking of the documentation. There are many columns that look quite similar but are entirely different measures and statistics. For example, within the demographics data there are multiple ways the racial makeup of a university are encoded. These columns appear to contain the same information drawn from perhaps different sources. However, the data is not consistent between sources and, at least within the demographics variables, contained large swaths of NA’s. Based on our subjective experience with the data, we assigned it a score of 4/10 because it could, theoretically, be worse. 

**Consistent Representation**
We calculated consistent representation by creating a function that collected the type for each row of a data frame. The function counts the number of unique types that occur in the data frame minus one (for scoring purposes). This number is then divided by the total number of types possible minus one. This number is subtracted from 1. If all of the data are encoded the same way, they will receive a score of 1. This is because the function will read that there is one unique type in the data frame. One will be subtracted from this, leading the function to divide 0/6, resulting in 0. 1 - 0 is 1. 1 was set as the highest score so that it would correspond with the other measures of data quality. If the data uses all 7 different kinds of data type that can be reasonably encoded, it will receive a score of 0. The Financials data set had three unique types, and received a score of .667. The Demographics data set had four unique data types, and received a score of 0.5.

```{r}

#CONSISTENCY OF REPRESENTATION FOR THE FINANCIALS DATA

paste("Score:", round(con_rep(fin),3)) 

```

```{r}

#CONSISTENCY OF REPRESENTATION FOR THE DEMOGRAPHICS DATA

paste("Score:", round(con_rep(dem),3)) 

```

**Completeness**
Completeness was measured by summing the total number of blank data rows in each column - blank referring to rows containing either NA, NaN, or Null - and dividing this by the total number of rows.  A score of 1 indicates perfect completeness. A score of 0 indicates no completeness, or a column of entirely null data. Within the Financial dataset, scores on completeness varied from 1, for the key values, to .195 for the unemployment rate. The mean completeness score over the entire dataset was 0.812, which indicates a moderate level of completeness. The Demographic dataset had a wider range of scores. Like the Financial dataset, the key values were awarded a completeness score of 1. However, response rates for the demographic variables had a wide distribution. The major races, such as White, Black, Hispanic, Asian, etc. had response rates of .983. However, response rates for ethnicities, such as White (non-hispanic) and Black (non-hispanic) had a response rate of 0. These variables were dropped during the cleaning process; however, they still skewed the mean completeness score for this variable. The mean completeness for the Demographic dataset was 0.658.

**Ease of Manipulation**
Ease of manipulation was quantified using the rules of coercion in R (as outlined in O'Reilly, R in a Nutshell, 2nd Edition). Since certain base R operations can only be performed on specific data types, the data type integrally influences how easily the data can be used and manipulated. The easier a data type is to coerce, or manipulate, into another data type, the higher a score it obtains. The gradient of scores moves from the logical (i.e., TRUE/FALSE) data type, which can be easily coerced into every other data types, to the raw data type, which cannot be implicitly coerced into other data types and is difficult to explicitly coerce. Because the logical data type can be universally manipulated, it scores a one. Since the raw data type cannot be coerced into another data type, it obtains an ease of manipulation score of zero. Rare data types, such as time series, were not included in the base version of this function because they are rarely included in data frames. Matrices were not included because it is nearly impossible to work them into a data frame, and because typeof returns their type as the type of data that is included within them. 

The ease of manipulation scores for both data sets are included below. The average ease of manipulation score for the Financials data set was 0.8, which indicates good ease of manipulation. The average ease of manipulation score for the Demographics data set was 0.825, which also indicates good ease of manipulation.

**FINANCIALS METRICS**

```{r, echo= FALSE}

fin_qual <- data_quality(fin)
rownames(fin_qual) <- fin_qual$ColumnName
fin_qual[,c(2,3)] %>% MakePretty()


```

```{r, echo = FALSE}
paste("Mean over columns:", mean(fin_qual$Ease_of_Manipulation))
```
**DEMOGRAPHICS METRICS**

```{r, echo = FALSE}
dem_qual <- data_quality(dem)
rownames(dem_qual) <- dem_qual$ColumnName
dem_qual[,c(2,3)] %>% MakePretty()

```

```{r, echo= FALSE}

paste("Mean over columns:", mean(dem_qual$Ease_of_Manipulation))

```

**Reputation**
    The data is well regarded as it has been used for other studies and was introduced by Obama in 2015. It highlighted the Pell grant problem and other financial issues being faced by college students across the country. Based on our subjective experience with the data, we assigned it a score of 8/10. It was not assigned a 10/10 because of some disputes about the quality of the data which have been raised by universities included in it. 

**Security**
    The data is highly secured as it was contained and dispersed by the United States Government. It is publicly accessible - however, this should not compromise the security of the data. There have been no known hackings of the data. We decided to assign the data a security score of 10/10. 

**Timeliness**
    The data is mostly up to date, but it would have been nice to have more recent statistics being recorded as financial data can change quickly as a product of circumstances; like a pandemic. Based on our subjective experience with the data, we assigned it a score of 9/10. 

<!--chapter:end:03-DataQuality.rmd-->

# Data Cleaning {#clean}

After the initial parsing of the data in the acquisition phase, there was not a significant amount of cleaning to be done. The cleaning stage primarily consisted of organizing the variables in a way that was intuitive to work with. We chose to handle NA's by replacing them with the group mean determined by their Carnegie classification.
For the demographics data set, we first dropped extraneous columns. These were columns which consisted of more than 90% NA values. Then, we renamed the variables so that they would be more intuitive to a general audience (and so that we could remember them better). There were four schools within the data that did not report any demographic variables for every year that they were in the data set. These schools were removed using drop_na(black), because the 'black' column did not possess any NAs other than for these four schools.

For both data sets, we chose to recode the Carnegie Classification variable (called r_status) as R1 and R2 to increase the interpretability of the data. These are converted to factors in the analysis section so that they can be input into the models.

## Demographics

```{r, message=FALSE}

#IMPORTING PACKAGES

library(tidyverse)

```


```{r, message = FALSE}

# READING IN THE CSV

dem <- read.csv("Proj1Data/Demographics15to19.csv")

# CLEANING THE DATA FRAME

dem_clean <- dem[,-c(1, 17, 20:24)] %>%
  rename("university" = instnm,
  "r_status" = ccbasic, 
  "state" = stabbr, 
  "white" = ugds_white, 
  "black" = ugds_black, 
  "hispanic" = ugds_hisp, 
  "asian" = ugds_asian, 
  "indigenous" = ugds_aian, 
  "nhpi" = ugds_nhpi, 
  "nra" = ugds_nra, 
  "unknown" = ugds_unkn) %>% 
  drop_na(black)

# RECODING THE CARNEGIE CLASSIFICATION (R_STATUS) VARIABLE

dem_clean$r_status <- recode(dem_clean$r_status, `15` = 'R1', `16` = "R2")

# NA HANDLING

dem_clean <- dem_clean %>%  
  group_by(r_status) %>%
  mutate_at(c("female", "first_gen", "poverty_rate", "veteran", "unemp_rate"), funs(ifelse(is.na(.), mean(., na.rm = TRUE),.)))

# WRITING TO A NEW CSV

write.csv(dem_clean, "Proj1Data/cleanDemographics1519.csv")

```

## Financials 

The Financials data set was easier to work with; as such, we simply renamed the common variables that we had renamed with demographics and re-coded the Carnegie Classification.

```{r, message= FALSE}

# READING IN THE CSV

fin <- read.csv('Proj1Data/Financials15to19.csv')

# RENAMING VARIABLES SO THAT THEY MATCH THE DEMOGRAPHIC DATA FRAME

fin_clean <- fin[,-c(2)] %>%
  rename("university" = instnm,
  "r_status" = ccbasic, 
  "state" = stabbr)

# RECODING THE CARNEGIE CLASSIFICATION (R_STATUS) VARIABLE

fin_clean$r_status <- recode(fin_clean$r_status, `15` = 'R1', `16` = "R2")

# WRITING TO A NEW CSV

write.csv(fin_clean, "Proj1Data/cleanFinancials1519.csv")

```

```{r}

#DISPLAYING THE NEW CSV

head(dem_clean, 5) 

```


<!--chapter:end:04-cleanDemographics.Rmd-->

# Methods and Results {#analysis}

```{r}
library(papeR)
library(kableExtra)
```


## Methods 

**Sample.**  
: The sample for this study consisted of universities in the United States of America. In this way, we were able to examine what was nearly the population of college students in America. However, the high frequency of NA variables in some categories hindered 

**Procedure.**
: The data was collected by accessing the public College Scorecard API. The procedure for extracting and cleaning the data is explained in the Data Extraction and Data Clean sections. 

**Analysis Strategy.**
: We quantified student outcomes using the median debt and unemployment rate variables. These variables were selected because they are tangibly related to financial outcomes. Other potential measures of success in the data, such as completion rates and death rates, contained either very sparse data or were ambiguous. For instance, neither transferring nor withdrawing from a University are necessarily indicative of a poor outcome (insofar as it relates to student success after attending the University). Both debt and unemployment, however, are. High rates of debt implicate a school with insufficient financial aid programs. High debt medians also potentially imply student’s are not placed into high-paying jobs that would allow them to expediently repay their debt. High rates of unemployment suggest that students have trouble finding jobs after graduation. As the goal of a university education is largely to educate a student to assist with their career, students being unable to find jobs is a poor outcome. 

: The demographic variables were selected by inspecting all the demographic variables in the dataset and selecting the ones that were mostly complete. We decided to look at the demographic makeup of R1 and R2 universities to determine if there was a significant difference between the student body profiles. Race and gender are often discussed in relation to student success, due to a history of systematic discrimination within the American university system. These were our primary variables of interest. Specifically, we were interested in the ratio of White students to minority students. However, we briefly examined veteran and first generation status. 


## Results

**SET-UP**

```{r, message=FALSE}

# IMPORTING PACKAGES

library(moments)
library(dplyr)
library(tidyr)
library(stats)
library(sjstats)
library(ggplot2)
library(plotly)
library(tidyverse)
library(purrr)
library(RColorBrewer)
library(tmap)

```

```{r}

# READING IN THE CSVs

fin <- read.csv('Proj1Data/cleanFinancials1519.csv')
cdem <- read.csv('Proj1Data/cleanDemographics1519.csv')
fin$r_status <- factor(fin$r_status, levels = c("R1", "R2"), labels = c(1,2))
cdem$r_status <- factor(cdem$r_status, levels = c("R1", "R2"), labels = c(1,2))

# TRANSFORMING THE DATASETS 

finR <- fin[fin$year == "latest",]
finR1 <- fin[fin$r_status == 1 & fin$year == "latest",]
finR2 <- fin[fin$r_status == 2 & fin$year == "latest",]
demR <- cdem[cdem$year == "latest",]
demR1 <- cdem[cdem$r_status == 1 & cdem$year == "latest",]
demR2 <- cdem[cdem$r_status == 2 & cdem$year == "latest",]
finR <- subset(finR, university %in% demR$university)

```

### Demographic Descriptive Statistics

**LOCATION OF R1'S vs. R2'S**

Before analyzing the demographic differences, we thought it was important to display how R1 and R2 schools are distributed across the United States. The demographic makeup of a school is potentially influenced by the demographics of in-state students which, at least at public institutions, are proportionally more likely to attend the school than students from any other state (see State Council of Higher Education for Virginia, 2019 for a case study of one state). Although we do not factor state and baseline state demographics into our model, it is a potential influential third variable that should be kept in mind while interpretting race and income demographics. 

```{r}

r1_state <- cdem %>%
  filter(r_status == "1") %>%
  group_by(state) %>%
  distinct(university) %>%
  count()

r2_state <- cdem %>%
  filter(r_status == "2") %>%
  group_by(state) %>%
  distinct(university) %>%
  count()

```

```{r}
g <- list(
  scope = 'usa',
  projection = list(type = 'albers usa'),
  lakecolor = toRGB('white')
)

r1_map <- plot_geo() %>%
  add_trace(
    z = ~r1_state$n,
    span = I(1),
    colorscale = 'Portland',
    zauto = F,
    zmax = 10,
    zmin = 1,
    locations = r1_state$state, 
    locationmode = 'USA-states'
  ) %>%
  colorbar(title = "# of Schools") %>%
  layout(geo = g, title = "Number of R1 Schools per State")

r2_map <- plot_geo() %>%
  add_trace(
    z = ~r2_state$n,
    span = I(1),
    colorscale = 'Portland',
    zauto = F,
    zmax = 10,
    zmin = 1,
    locations = r2_state$state, 
    locationmode = 'USA-states'
  ) %>%
  colorbar(title = "# of Schools") %>%
  layout(geo = g, title = "Number of R2 Schools per State")

r1_map
r2_map
```

**RACE**

The prelimary examination of the demographics data revealed that all of the race values, except for the proportion of white students, had a significant right skew. While the distributions are not normal, they are symmetric and bounded, which prevents extreme outliers. Below we provide a violin plot showing the distribution for each of the race variables grouped by their Carnegie classification. The mean is marked by a red bar, and the median is marked by a black bar. 

```{r, fig.height = 10}

dem_pal <- c("darkolivegreen", "darkolivegreen3", "dodgerblue4", "deepskyblue", "lavenderblush4", "lavenderblush2", "palevioletred1", "rosybrown1",  "tomato2",  "sienna 1", "slateblue3", "thistle1", "orange", "navajowhite2", "lightskyblue3", "lightsteelblue1")
  
race_demographics <- cdem %>%
  select(university, r_status, white, black, hispanic, asian, indigenous, nhpi, nra, unknown) %>%
  gather("race", "percentage", 3:10, -university)

race_demographics %>%
  ggplot(aes(x = factor(r_status), y = percentage, fill = interaction(r_status, race))) +
  geom_violin() + 
  stat_summary(fun=mean, geom="crossbar", linetype = 1, size=.2, color = "red") + 
  stat_summary(fun=median, geom="crossbar", linetype = 1, size=.2, color = "black") +
  labs(title = "Racial Demographics by Carnegie Classification", 
       subtitle = "Red line = mean | Black line = median\n",
       x = " ", 
       y = "% of Student Body") + 
  facet_wrap(~as.factor(race), nrow = 5) + 
  scale_fill_manual(values = dem_pal) + 
  theme_gray() + 
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)), 
        plot.title = element_text(hjust = .5), 
        legend.position = "none")

```


The five number summary for the 'white variable', along with the skewness and kurtosis, are reported below. 

```{r}

# DEMOGRAPHICS AT R1'S - WHITE STUDENTS 

summary(demR1$white)
skewness(demR1$white)
kurtosis(demR1$white)

```


```{r}
# DEMOGRAPHICS AT R2'S - WHITE STUDENTS 
summary(demR2$white)
skewness(demR2$white)
kurtosis(demR2$white)
```

**OTHER DEMOGRAPHICS**

The non-racial variables (i.e., gender, first generation status, veteran status) did not posess the same skew. However, the mean was inflated due to how we coded NA's resulting in peculiar looking violin plots.  

```{r, fig.height = 10, fig.width = 6}

#Demographics for sparse columns 

other_demographics <- cdem %>%
  select(university, r_status, female, first_gen, veteran) %>%
  gather("demographic", "percentage", 3:5, -university)

other_plot <- other_demographics %>%
  ggplot(aes(x = factor(r_status), y = percentage, fill = interaction(r_status, demographic))) +
  geom_violin() + 
  stat_summary(fun=mean, geom="crossbar", linetype = 2, size=.1, color = "red") + 
  labs(title = "Other Demographic Factors by Carnegie Classification", 
       subtitle = "",
       x = " ", 
       y = "% of Student Body") + 
  facet_wrap(~as.factor(demographic), nrow = 5) + 
  scale_fill_brewer("Paired") + 
  theme_gray() + 
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)), 
        plot.title = element_text(hjust = .5), 
        legend.position = "none")

other_plot

```


### Financial Descriptive Statistics

The code for the first histogram is displayed, the rest will be hidden to avoid unnecessary repitition. 

```{r}
# Cost
print('Tuition cost')
summary(finR1$costt4_a)
cost_hist1 <- ggplot(finR1, aes(x =costt4_a)) + 
  geom_histogram(bins = 40, color = "black", fill = "red") +
  labs(title = "Distribution of Tuition Costs at R1s", 
       subtitle = "", 
       x = "Cost (in American dollars)", 
       y = "Frequency") 
cost_hist1
```

```{r echo = FALSE}
summary(finR2$costt4_a)
cost_hist2 <- ggplot(finR2, aes(x =costt4_a)) + 
  geom_histogram(bins = 40, color = "black", fill = "pink") +
  labs(title = "Distribution of Tuition Costs at R2s", 
       subtitle = "", 
       x = "Cost (in American dollars)", 
       y = "Frequency") 
cost_hist2
```

```{r, echo = FALSE}
# Students debt
print('Student debt')
summary(finR1$grad_debt_mdn)
debt_hist1 <- ggplot(finR1, aes(x = grad_debt_mdn)) + 
  geom_histogram(bins = 40, color = "black", fill = "cadetblue4") +
  labs(title = "Distribution of Median Student Debt at R1s", 
       subtitle = "", 
       x = "Median Debt (in American dollars)", 
       y = "Frequency") 
debt_hist1
```

```{r, echo = FALSE}
summary(finR2$grad_debt_mdn)
debt_hist1 <- ggplot(finR2, aes(x = grad_debt_mdn)) + 
  geom_histogram(bins = 40, color = "black", fill = "cadetblue3") +
  labs(title = "Distribution of Median Student Debt at R2s", 
       subtitle = "", 
       x = "Median Debt (in American dollars)", 
       y = "Frequency") 
debt_hist1
```

**Unemployment**

```{r}

print('Unemployment Rate')
summary(finR1$unemp_rate)
unemp_hist1 <- ggplot(finR1, aes(x = unemp_rate)) + 
  geom_histogram(bins = 40, color = "black", fill = "darkseagreen4") +
  labs(title = "Distribution of Post-Graduation Unemployment rates at R1s", 
       subtitle = "", 
       x = "Unemployment Rate", 
       y = "Frequency") 
unemp_hist1
```

```{r}

summary(finR2$unemp_rate)
unemp_hist2 <- ggplot(finR2, aes(x = unemp_rate)) + 
  geom_histogram(bins = 40, color = "black", fill = "darkseagreen3") +
  labs(title = "Distribution of Post-Graduation Unemployment rates at R2s", 
       subtitle = "", 
       x = "Unemployment Rate", 
       y = "Frequency") 
unemp_hist2

```

### Significance Testing

Preliminary normality analysis of the median graduate debt distribution revealed levels of skewness that forbade the use of parametric comparison tests. A Wilcoxon signed-rank test was conducted in order to investigate the differences in median graduate debt between R1 and R2 institutions. Results indicate that median graduate debt significantly differed between R1 and R2 institutions (W = 982, p < 0.01). While a difference in student debt was expected due to the aforementioned price of R1 institution tuition, descriptive statistics revealed graduates of R1 institutions graduated with less average debt (M = 19,652, SD = 3,750) than students from R2 institutions (M = 22,719, SD = 3,785).  

```{r}

wilcox.test(formula=finR$grad_debt_mdn~finR$r_status)

```

In order to further explore student outcomes, the investigators turned their attention to the graduate unemployment rate. The distribution for unemployment exhibited skewness and kurtosis values within acceptable bounds so a Welch’s two-sample t-test was conducted in order to find whether there were significant differences in unemployment rates between R1 and R2 graduates. The test revealed significant differences in rates of unemployment between R1 and R2 graduates t(118.36) = -2.6055, p = 0.01, 95%C.I. [-0.44, -0.06]. On average, the unemployment rate for R1 graduates (M = 3.293, SD = 0.51) was lower than that observed for R2 graduates (M = 3.545, SD = 0.83) 

```{r}

t.test(finR$unemp_rate~finR$r_status)

```

### Regression model  

The racial diversity proportion of an institution was examined as a potential moderator of the relationship between institution type (R1 vs. R2) and unemployment rate. In the first step of the regression analysis institution type was entered into the model and accounted for  5% of the variance in unemployment rate, which was significant, R2= 0.52, F(1,122) = 6.82, p = 0.01.  In the second step of the regression analysis the interaction effect between institution type and racial diversity proportion were entered into the model and explained an additional 4% of the variance in unemployment rate, which was significant, R2= 0.04, F(1,121) = 8.57, p < 0.05. Racial diversity proportion was entered into the model last and accounted for 24% of the variance in unemployment rate, which was significant, R2= 0.24, F(1,122) = 38.8 , p < 0.01. Thus, racial diversity proportion was a significant moderator of the relationship between institution type and unemployment rate.  

*Note:* Below the code for each model is a statistical summary of the model made using the [{papeR}](https://cran.r-project.org/web/packages/papeR/vignettes/papeR_introduction.html) package for data formatting and [{kableExtra}](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html) for clean presentation. This code is hidden for cleanliness of the document

```
pretty_lm <- prettify(summary(lm)) #prettify() from papeR
kable(pretty_lm)                    #kable() from kableExtra
```

**Model step 1**

```{r}
predictor <- finR$r_status
outcome <- finR$unemp_rate

simplelm <- lm(formula = outcome ~ predictor)
```

```{r, echo = FALSE}
pretty_simplelm <- prettify(summary(simplelm))
kable(pretty_simplelm)
```
**Model step 2**

```{r}
predictor <- demR$white
outcome <- finR$unemp_rate

moderatorlm <- lm(formula = outcome ~ predictor)
```

```{r, echo = FALSE}
pretty_moderatorlm <- prettify(summary(moderatorlm))
kable(pretty_moderatorlm)
```

**Model step 3**

```{r}
predictor <- as.integer(finR$r_status)
moderator <- demR$white
interaction <- predictor * moderator
outcome <- finR$unemp_rate

interactionlm <- lm(formula = outcome ~ interaction)

```

```{r, echo = FALSE}
pretty_interactionlm <- prettify(summary(interactionlm))
kable(pretty_interactionlm)
```

**Complete model**

```{r}
predictor <- as.integer(finR$r_status)
moderator <- demR$white
interaction <- predictor * moderator
outcome <- finR$unemp_rate

wlm <- lm(formula = outcome ~ predictor + moderator + interaction)
```

```{r, echo = FALSE}
pretty_lm <- prettify(summary(wlm))
kable(pretty_lm)
```


<!--chapter:end:05-Analysis.Rmd-->

# Discussion {#discussion}

## Discussion

  These results are meant to serve as illustration of the data in the College Scorecard API pertaining to schools at the highest levels of Carnegie Classification. In addition to the descriptive analysis, which demonstrates the distributions of demographics and financial data, our regression analysis demonstrates that the distinction between R1 or R2 classifications has a significant, albeit small effect on unemployment rates for graduates from these universities. Interestingly, the relationship between unemployment and School type was moderated by the proportion of white people at a given university. Commentary on the social conditions surrounding this relationship is beyond the scope of this project and the researchers’ expertise; nevertheless, we hope these results serve others in discussions of systemic issues related to race and higher education.

## The Project Process
    
  We should admit that at the onset of this project our team was lost in the complexity of the data in the College Scorecard API. This was largely due to our own stubbornness, as we wanted to use the API because of the advantages this method offers in regards to reproducibility. That being said, our project scope began to grow exponentially after learning about the rscorecard package, which not only offered a highly interpretable method for extracting the data but also offered further insight into how the data from the API was structured in and of itself.     
Prior to developing a plan for our analysis (or even a direction for the project) we began by setting up the repositories and collaborative documents that we planned to use throughout the project. The primary collaborative tools were GitHub, Google Docs, and Discord. We used Discord to keep in contact because of their native support for markdown as part of their messaging service, which allowed us to send code chunks directly to each other, as well as the voice-conferencing and screen sharing functionality that allowed us to meet and discuss progress. The Google Docs were primarily used for planning and drafting the written portions of our project. Lastly, we used github to share files with one another and used the gitpages feature to host the final document as a web page. 

The documents in the [CollegeScorecardProjectBook](https://github.com/KalaniStanton/CollegeScorecardProjectBook) repository contain all of the code, data, and configuration files needed in order to knit the bookdown on each team member’s computer. Ultimately, this means that a cloned repository of our GitHub should run and knit, just as long as the user acquires their own API key and installs all the necessary dependencies into their RStudio environment.
In truth, this project suffered from quite a bit of “feature creep” once we began working on the project in earnest. As we finally began to understand the data, we realized there were a lot of ways that we could take full advantage of the information at our disposal using the specialized skill sets of our team members. Consequently, we were unable to discuss all of the visualizations that we had made during this process, but have included them in case anyone would like to use our code as reference for their own exploratory projects in the future.

## Things Learned & Skills Acquired

### Adam
I was incredibly surprised by how much we were able to glean from only a handful of variables of interest. We definitely came into the exploratory analysis ‘laptops blazing’ to burn down the ivory R1 throne but alas the dataset managed to both defuse our animosity and reveal a much more realistic painting of R1 characteristics. I feel that we were able to learn alot about the diversity characteristics and outcomes of R1 vs R2 schools in spite of the project’s fairly narrow scope. On a different note, I really enjoyed getting to work with holistically skewed and messy data. On a longer time scale I would have loved to seek out and model some of the more obscure latent variables and their relation to student outcomes. 

>“people use statistics the same way that a drunk uses a lamppost. 
>   More for support than for illumination.”
                      Andrew Lang, a long ass time ago

### Kalani
  
Despite my previous endeavors with Bookdown (I had knit one years ago), I struggled to navigate the highly technical documentation available for this package. Nevertheless, I persisted and was able to create a clean document format that parallelled the fantastic work put in by my other team members. Through Adam’s work on statistical analysis, I became re-acquainted with some of the modeling and statistical tools I had used in my undergrad data science and psychology classes, but had also learned more about experimental design from Adam himself. I also collaborated with Sara to make the interactive chloropleth maps using plotly, which I had never done before and am happy I took the time to learn. All in all, I’m really pleased with this project and appreciated the diverse skill set across the members of this team. 

> "There is no such thing as a 'good' programmer; 
>   just a competent googler."
                         ~Unknown

### Sara 
Most of what I learned from this project I learned from my partners. Kalani's Bookdown and data-wrangling ingenuity and Adam's statistics proficiency added layers to the learning experience, and made the assignment fun to work on. I also enjoyed reading Ben’s interpretation of the subjective data quality features.  I was impressed by how much we were able to do with a fairly sparse and limited data set. We managed to probe at a topic I find genuinely interesting, and scrape insights from below the surface level of the data. I also enjoyed being able to implement the data quality functions I created for the homework assignment on actual data sets. It provided meaning to the work we had completed so far. 

### Ben

This group project gave us all some much-needed group work experience as well as sharpened our technical skills in R and Git. We collaborated using a Git Repository and Kalani used Bookdown for the formal writeup. It was an interesting experience, and we all were able to contribute in our own way. As for what we learned, the dataset showed us real data about the distribution and diversity of colleges across the United States. When we usually think about these institutions, we come in with our own pre -conceived biases and opinions but when we look at the actual data, we can begin to formulate real inferences and conclusions. In our other works, we haven’t had this experience yet, so it was a nice change of pace.

## Other fun things 

Features that we were interested in, but didn't get a chance to follow through on or did not align with the primary goal of the project. :)

### Gender 

```{r}

#Universities with the highest female/male ratio, the top 10 are mostly R2's for both categories

cdem %>%
  arrange(desc(female)) %>%
  filter(female > .50) %>%
  distinct(university, .keep_all = TRUE) %>%
  head(10)

cdem %>%
  arrange(female) %>%
  filter(female < .50) %>%
  distinct(university, .keep_all = TRUE) %>%
  head(10)

R1s <- cdem %>%
  filter(r_status == 1)

ggplot(cdem, aes(x = female)) + 
  geom_histogram(bins = 40, color = "black", fill = "pink") + 
  labs(title = "Distribution of Female/Male Gender ratio at R1s")

R2s <- cdem %>%
  filter(r_status == 2)

ggplot(R2s, aes(x = female)) + 
  geom_histogram(bins = 40, color = "black", fill = "skyblue") + 
  labs(title = "Distribution of Female/Male Gender ratio at R2s")

```

<!--chapter:end:06-Discussion.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}

'`
```{r, messages = FALSE, warning=FALSE}
#install.packages("devtools")
#devtools::install_github("neuropsychology/report")
library(report) #Package for citing
```

```{r}
kable(cite_packages(sessionInfo())) %>% kable_styling(full_width = F)
```

<!--chapter:end:07-references.Rmd-->

