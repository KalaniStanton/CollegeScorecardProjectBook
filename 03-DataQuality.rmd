# Data Quality {#quality}

## Set-up

First, I import the packages that I use in this section and read in the uncleaned data.

```{r, message = FALSE}
#LIBRARIES 
library(tidyverse)
library(kableExtra)
MakePretty <- function(x) {
  x %>% kbl(align = "c") %>% kable_material()
}
```

```{r, message = FALSE}

#Reading in the data
fin <- read.csv('Proj1Data/Financials15to19.csv')
fin <- fin[,-1]

dem <- read.csv("Proj1Data/Demographics15to19.csv")
dem <- dem[,-1]

```

## The Functions 

**CONSISTANT REPRESENTATION**

```{r}

con_rep <- function(df){
  "
  A function that quantitatively scores an input data frame on the consistancy of representation data quality metric.
  
  Input: 
    df: a data frame
  Output: 
    con_rep_score: A numeric score on consistency of representation ranging from 1 to 0, where 1 is perfectly consistent representation and 0 is inconsistent representation.
  "
  
  type = vector()
  for(i in 1:ncol(df)){
    col_type <- typeof(df[1,i])
    type[i] <- col_type
  }
  
  con_rep_score <- 1 - ((length(unique(type)) - 1)/6)
  return(con_rep_score)
}

```

**COMPLETENESS AND EASE OF MANIPULATION**

```{r}

data_quality <- function(df){
  
  "
  A function to quantitatively compute scores for a dataframe on the completeness and ease of manipulation data quality metrics. 
  
  Input: 
    df: A data frame
    
  Output: 
    qualityTable: A table reporting the scores on completeness and ease of manipulation for each column in the input data frame. 
  "
  
  # Setting the index value, which will be used to index the column name 

  index <- 1
  
  # Instantiating empty data frames for each of the queries
  
  completeness <- data.frame(Completeness=double())
  eom <- data.frame(Ease_of_Manipulation=double())
  names <- data.frame(ColumnName=character())
  
  # Populating the data frames using a for-loop
  
  for (i in df){
    
    # COLLECTING THE NAMES OF EACH COLUMN PASSED
    
    col <- colnames(df[index])
    
    # COMPLETENESS
    # Takes the sum of the total NA, NULL, and NaN values in a column
    # Divides them by the length of the column
    # Subtracts this from one, as was suggested by Pipinio, Lee, and Wang
    # And then rounds to output to the third decimal place
    
    c <- 1-(sum(is.na(i) + is.null(i) + is.nan(i))/length(i)) %>%
      round(digits = 3)
    
    # EASE OF MANIPULATION
    # "Case when" vectorises a series of if/else statements
    # The function checks the type of the column and then sets the variable,
    # e, to the corresponding value. 
    
    e <- case_when(
      typeof(i) == "logical" ~ 1,
      typeof(i) == "integer" ~ .9,
      typeof(i) == "numeric" ~ .8,
      typeof(i) == "double" ~ .8,
      typeof(i) == "complex" ~ .7,
      typeof(i) == "character" ~ .6,
      typeof(i) == "list" ~ .5, 
      typeof(i) == "raw" ~ 0,
      TRUE ~ 0)
    
    #The index used to collect column names is increased by one
    
    index = index + 1
    
    #Appending the output for each column to their respective data frames
    
    completeness[nrow(completeness)+1,] <- c
    eom[nrow(eom)+1,] <- e
    names[nrow(names)+1,] <- col
  }
  
  #Binding the columns of the three tables into an output table
  qualityTable <- cbind(names, completeness, eom)
  
  return(qualityTable)
}

```


## The Evaluations 

We assessed data quality using the metrics outlined in (paper). For each of these metrics, we provide a brief commentary on how the data fared. Several of the data quality metrics were more pertinent to our analysis, so we provide deeper insight into them.

**Accessibility**
On its own, the data were not accessible and extraction proved to be exceedingly challenging. By using the {rscorecard} package, we were able to streamline the process. However, {rscorecard} is not built into the API and cannot be factored into the dataset's baseline accessibility. Despite the challenge of pulling the data into an interpretable form, they can be accessed by the public. This is to say, we did not have to fight with a Facebook executive to access them.  We scored the data at a 4/10 for accessibility.

**Believability**
There is no reason for us to assume the data would be intentionally falsified as it was collected by the U.S. Government to simply compare costs and values of higher education institutions. There have been claims from some colleges (such as Boston University) that there are inaccuracies in the data, however. We decided to give the data a score of 8/10 on believability. 

**Concise Representation**
The data is difficult to navigate through without frequent cross-checking of the documentation. There are many columns that look quite similar but are entirely different measures and statistics. For example, within the demographics data there are multiple ways the racial makeup of a university are encoded. These columns appear to contain the same information drawn from perhaps different sources. However, the data is not consistent between sources and, at least within the demographics variables, contained large swaths of NAâ€™s. Based on our subjective experience with the data, we assigned it a score of 4/10 because it could, theoretically, be worse. 

**Consistent Representation**
We calculated consistent representation by creating a function that collected the type for each row of a data frame. The function counts the number of unique types that occur in the data frame minus one (for scoring purposes). This number is then divided by the total number of types possible minus one. This number is subtracted from 1. If all of the data are encoded the same way, they will receive a score of 1. This is because the function will read that there is one unique type in the data frame. One will be subtracted from this, leading the function to divide 0/6, resulting in 0. 1 - 0 is 1. 1 was set as the highest score so that it would correspond with the other measures of data quality. If the data uses all 7 different kinds of data type that can be reasonably encoded, it will receive a score of 0. The Financials data set had three unique types, and received a score of .667. The Demographics data set had four unique data types, and received a score of 0.5.

```{r}

#CONSISTENCY OF REPRESENTATION FOR THE FINANCIALS DATA

paste("Score:", round(con_rep(fin),3)) 

```

```{r}

#CONSISTENCY OF REPRESENTATION FOR THE DEMOGRAPHICS DATA

paste("Score:", round(con_rep(dem),3)) 

```

**Completeness**
Completeness was measured by summing the total number of blank data rows in each column - blank referring to rows containing either NA, NaN, or Null - and dividing this by the total number of rows.  A score of 1 indicates perfect completeness. A score of 0 indicates no completeness, or a column of entirely null data. Within the Financial dataset, scores on completeness varied from 1, for the key values, to .195 for the unemployment rate. The mean completeness score over the entire dataset was 0.812, which indicates a moderate level of completeness. The Demographic dataset had a wider range of scores. Like the Financial dataset, the key values were awarded a completeness score of 1. However, response rates for the demographic variables had a wide distribution. The major races, such as White, Black, Hispanic, Asian, etc. had response rates of .983. However, response rates for ethnicities, such as White (non-hispanic) and Black (non-hispanic) had a response rate of 0. These variables were dropped during the cleaning process; however, they still skewed the mean completeness score for this variable. The mean completeness for the Demographic dataset was 0.658.

**Ease of Manipulation**
Ease of manipulation was quantified using the rules of coercion in R (as outlined in O'Reilly, R in a Nutshell, 2nd Edition). Since certain base R operations can only be performed on specific data types, the data type integrally influences how easily the data can be used and manipulated. The easier a data type is to coerce, or manipulate, into another data type, the higher a score it obtains. The gradient of scores moves from the logical (i.e., TRUE/FALSE) data type, which can be easily coerced into every other data types, to the raw data type, which cannot be implicitly coerced into other data types and is difficult to explicitly coerce. Because the logical data type can be universally manipulated, it scores a one. Since the raw data type cannot be coerced into another data type, it obtains an ease of manipulation score of zero. Rare data types, such as time series, were not included in the base version of this function because they are rarely included in data frames. Matrices were not included because it is nearly impossible to work them into a data frame, and because typeof returns their type as the type of data that is included within them. 

The ease of manipulation scores for both data sets are included below. The average ease of manipulation score for the Financials data set was 0.8, which indicates good ease of manipulation. The average ease of manipulation score for the Demographics data set was 0.825, which also indicates good ease of manipulation.

**FINANCIALS METRICS**

```{r, echo= FALSE}

fin_qual <- data_quality(fin)
rownames(fin_qual) <- fin_qual$ColumnName
fin_qual[,c(2,3)] %>% MakePretty()


```

```{r, echo = FALSE}
paste("Mean over columns:", mean(fin_qual$Ease_of_Manipulation))
```
**DEMOGRAPHICS METRICS**

```{r, echo = FALSE}
dem_qual <- data_quality(dem)
rownames(dem_qual) <- dem_qual$ColumnName
dem_qual[,c(2,3)] %>% MakePretty()

```

```{r, echo= FALSE}

paste("Mean over columns:", mean(dem_qual$Ease_of_Manipulation))

```

**Reputation**
    The data is well regarded as it has been used for other studies and was introduced by Obama in 2015. It highlighted the Pell grant problem and other financial issues being faced by college students across the country. Based on our subjective experience with the data, we assigned it a score of 8/10. It was not assigned a 10/10 because of some disputes about the quality of the data which have been raised by universities included in it. 

**Security**
    The data is highly secured as it was contained and dispersed by the United States Government. It is publicly accessible - however, this should not compromise the security of the data. There have been no known hackings of the data. We decided to assign the data a security score of 10/10. 

**Timeliness**
    The data is mostly up to date, but it would have been nice to have more recent statistics being recorded as financial data can change quickly as a product of circumstances; like a pandemic. Based on our subjective experience with the data, we assigned it a score of 9/10. 
