[["index.html", "An Exploratory Analysis of Data in the U.S. College Scorecard Preface", " An Exploratory Analysis of Data in the U.S. College Scorecard Sara Haman, Adam Lashley, Reilly Stanton, Benjamin Weisman 2020-10-19 Preface This project was completed to satisfy the requirements of a project assigned to students in the Fall 2020 Data Munging and Exploratory Data Analysis at New College of Florida. This document is broken down into sections pertaining to the discrete steps in the process of exploratory data analysis. In the first chapter (1), we present an overview of this project. In the second chapter (2), we outline how we accessed and cleaned the data. We demonstrate our initial attempts at pulling the data directly from the API, and explain why we chose to pivot to using the {rscorecard} package for querying our data from the API. In the third (3), we compute scores on each of the data quality metrics described in Pipino, Lee, and Wang (2002). Although we handle NAs while cleaning the data, the data quality metrics are performed on the initial data pulls, so that our assessment procedure is generally reproducible. The fourth chapter (4) provides the code used to clean the data. Then, we begin the analysis of the data (5). This is done in two steps: first, by exploring the descriptive variables in order to glean the composition of the data set, and second by investigating our theory that the proportion of White students at a school will positively influence student outcomes (possibly due to implicit favor given to White individuals). We examine features that R1 and R2 schools differ on, and provide a model for the relationship between demographics and student outcomes. The results of these tests are reported (5). In this discussion (6), we summarize the key findings from the analysis. "],["intro.html", "Section 1 Introduction", " Section 1 Introduction The Carnegie Classification system classifies American universities by their type and their research productivity. Type is determined by the range of degrees a university awards. Institutions which award at least 20 research-focused doctoral degrees a year are categorized as doctoral universities. Within doctoral universities, schools are given secondary ratings commensurate with the extent of their research expenditures. These ratings, which include R1, R2, and D/PU, are colloquially referenced as a measure of prestige. R1 schools, or schools with very high research activity, are prototypically represented by old and infamous universities, such as Harvard, and flagship state institutions. However, an R1 designation is not indicative of the quality of research produced, nor the caliber of education one receives at the school. The two factors which influence a schools R status are the ratio of doctoral degrees awarded over faculty count, and the monetary amount the school invests into research. This is to say, an institution could conduct horrible, hackneyed research that never gets published except in predatory journals and still earn an R1 designation if they spend at least 5 million dollars on research expenditures (The Carnegie Classification of Institutes of Higher Education, 2018). Of course, this situation is not realistically feasible. Donors would abandon a school producing blatantly unproductive research. Yet, the reputation of a school is, at some level, intertwined with their R designation. When Dartmouths designation was decremented in 2016, media outlets reported on it as if it were shameful (Anderson, 2016). Without context for how Carnegie Classifications are assigned, the R1 designation may seem superior to R2. Our group chose to investigate the influence of the Carnegie Classification of an institution on student outcomes; essentially, if attending an R1 school provides a tangible benefit over attending an R2. Furthermore, we explore features of R1 and R2 universities to determine which variables they significantly differ on. "],["import.html", "Section 2 Data Acquisition 2.1 Set-up and API Exploration 2.2 The rscorecard and tidyverse approach", " Section 2 Data Acquisition library(plyr) library(tidyverse) library(Hmisc) library(httr) library(jsonlite) library(tidyverse) #install.packages(&quot;tmap&quot;) library(tmap) library(leaflet) library(treemap) library(kableExtra) `%notin%` &lt;- Negate(`%in%`) MakePretty &lt;- function(x) { x %&gt;% kbl(align = &quot;c&quot;) %&gt;% kable_material() } 2.1 Set-up and API Exploration For our project, we wanted to practice using API to pass the data into our global environment and attempt to explore and extract our desired variables using base R. As you will see, our attempt failed, but was useful in helping us understand the relational structure of the College Scorecard Data and the limits of simple indexing when dealing with complicated data structures. 2.1.1 Project Directory Management Prior to extracting any of the data, we want to ensure that our project is reproducible. Thus, we first create a sub-directory within the users working directory, which will hold the .csv files we will write containing our variables of interest. proj.dir &lt;- getwd() #dir.create(&quot;Proj1Data&quot;) data.dir &lt;- paste0(proj.dir, &quot;/Proj1Data&quot;) Next, we pass the API key into an object so that it can be called in the GET() function, which pulls the data from the API into out global environment. APIKey = &quot;FELgrGb47PaevTWxqZTt6etFaQVnDbKpcJLaPL6a&quot; res = GET(paste0(&quot;https://api.data.gov/ed/collegescorecard/v1/schools?api_key=&quot;, APIKey)) This data is initially in a raw format that is pulled from a JSON file stored on the API servers. To do this, we first convert the raw data to char and call the fromJSON() function to convert the data to a more R friendly data structure. data = fromJSON(rawToChar(res$content)) is(data) ## [1] &quot;list&quot; &quot;vector&quot; Now that this raw JSON file is converted to a list, we can begin our exploration of the data. 2.1.2 API Exploration We originally intended on using list2env(yearsLi, envir = .GlobalEnv) to split the list into multiple data.frame objects, but the fact that the names of the listed data frames are numeric values will be a problem. When calling these data frames, R will have to decide whether the user input 2012 is calling the data$2012 data set, or the number 2012. This is likely to cause problems in our later analysis, so we have to look at the names() of objects in our data list to see how we might adjust our approach to exploring the data. names(data$results) ## [1] &quot;2012&quot; &quot;2011&quot; &quot;2010&quot; &quot;2009&quot; &quot;1998&quot; &quot;2008&quot; ## [7] &quot;1997&quot; &quot;2007&quot; &quot;1996&quot; &quot;2006&quot; &quot;2005&quot; &quot;school&quot; ## [13] &quot;2004&quot; &quot;2003&quot; &quot;2002&quot; &quot;id&quot; &quot;latest&quot; &quot;1999&quot; ## [19] &quot;2001&quot; &quot;2000&quot; &quot;2018&quot; &quot;ope6_id&quot; &quot;2017&quot; &quot;2016&quot; ## [25] &quot;2015&quot; &quot;2014&quot; &quot;2013&quot; &quot;ope8_id&quot; &quot;location&quot; Upon looking at the names() we noticed that not all of the listed data frames are named after years, and thus are likely contain different data than the others. Therefore, we first extract the non-year data frames, prior to extracting the year data frames with a character alteration (to make them non-numeric), id_data &lt;- data$results[c(&quot;school&quot;, &quot;id&quot;, &quot;location&quot;, &quot;ope6_id&quot;, &quot;ope8_id&quot;)] yearsLi&lt;- data$results[names(data$results) %notin% c(&quot;school&quot;, &quot;id&quot;, &quot;location&quot;, &quot;ope6_id&quot;, &quot;ope8_id&quot;)] names(yearsLi) &lt;- paste0(&quot;yr&quot;, names(yearsLi)) #list2env(yearsLi, envir = .GlobalEnv) At this point, we then called list2env(data$results, envir = .GlobalEnv) to create individual dataframes of each object in the yearsLi list, which helped us understand what was happening at different levels of the data structure, but ultimately left us more confused about how we should approach querying this data. This code, as well as the code we used to analyze these objects below, is commented it out because it creates an unnecessary number of objects in the global environment, but retained in this document to show the attempts we made at exploring the data structure. # #names(yr2012) #contents(id_data$school) #contents(yr2012$academics$program$bachelors) #yr2012$academics$program$bachelors #length(id_data$ope8_id) At this point, the challenge of collecting data directly from the API seemed insurmountable, but we sought a solution and turned to the sage of all data science obstacles: Google. 2.2 The rscorecard and tidyverse approach As expected, we found out savior in the annals of algorithmic wisdom when we stumbled upon a package called {rscorecard}. The {rscorecard} package is a wrapper for the College Scorecard API that takes full advantage of the complexity in this relational data structure by employing useful tools from the tidyverse eco-system. Using {dplyr} like functions, specified to this data set, and pipe operators %&gt;% from {magrittr}, the rscorecard package provides an astonishingly simple solution for querying data directly from the API. #install.packages(&quot;rscorecard&quot;) library(rscorecard) To access the API in this package we pass the object holding our API Key [as a string] into the sc_key() function. sc_key(APIKey) As the message above states, once the API key is set, the user can append conditions to the query function sc_get() to extract the data they desire. These conditions are applied using functions in the package, which are appended to the query function using the aforementioned pipes. The primary functions for conditioning the queries are sc_filter(), to collect observations that satisfy boolean expressions, sc_select(), to subset the data by columns, and sc_year(), to select the data from a specified year. These functions are what we use below to extract data on Finances, Demographics, and Death Rates from schools that classify as R1 and R2 research institutions. 2.2.1 Finances F_latest &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr, npt4_pub, npt4_priv, costt4_a, grad_debt_mdn, avgfacsal, unemp_rate) %&gt;% sc_year(&quot;latest&quot;) %&gt;% sc_get() F_2018 &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr, npt4_pub, npt4_priv, costt4_a, grad_debt_mdn, avgfacsal, unemp_rate) %&gt;% sc_year(2018) %&gt;% sc_get() F_2017 &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr, npt4_pub, npt4_priv, costt4_a, grad_debt_mdn, avgfacsal, unemp_rate) %&gt;% sc_year(2017) %&gt;% sc_get() F_2016 &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr, npt4_pub, npt4_priv, costt4_a, grad_debt_mdn, avgfacsal, unemp_rate) %&gt;% sc_year(2016) %&gt;% sc_get() F_2015 &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr, npt4_pub, npt4_priv, costt4_a, grad_debt_mdn, avgfacsal, unemp_rate) %&gt;% sc_year(2015) %&gt;% sc_get() F_latest %&gt;% mutate(year = &quot;latest&quot;) ## # A tibble: 266 x 11 ## control instnm ccbasic stabbr npt4_pub npt4_priv costt4_a grad_debt_mdn ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 India~ 16 IN 10987 NA 20704 22343 ## 2 1 Unive~ 15 KS 18571 NA 24996 21105 ## 3 2 Bosto~ 15 MA NA 33562 70588 16939 ## 4 1 Oakla~ 16 MI 11560 NA 21231 25000 ## 5 1 Unive~ 15 MS 13857 NA 24822 19500 ## 6 1 Misso~ 16 MO 14205 NA 22012 24750 ## 7 2 Washi~ 15 MO NA 27427 71975 16075 ## 8 1 Monta~ 15 MT 18036 NA 20464 23500 ## 9 1 Unive~ 16 NE 13511 NA 18849 20250 ## 10 1 Unive~ 15 NV 10551 NA 17582 18750 ## # ... with 256 more rows, and 3 more variables: avgfacsal &lt;int&gt;, ## # unemp_rate &lt;dbl&gt;, year &lt;chr&gt; F_2018 %&gt;% mutate(year = 2018) ## # A tibble: 266 x 11 ## control instnm ccbasic stabbr npt4_pub npt4_priv costt4_a grad_debt_mdn ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt; ## 1 1 India~ 16 IN 10987 NA 20704 NA ## 2 1 Unive~ 15 KS 18571 NA 24996 NA ## 3 2 Bosto~ 15 MA NA 33562 70588 NA ## 4 1 Oakla~ 16 MI 11560 NA 21231 NA ## 5 1 Unive~ 15 MS 13857 NA 24822 NA ## 6 1 Misso~ 16 MO 14205 NA 22012 NA ## 7 2 Washi~ 15 MO NA 27427 71975 NA ## 8 1 Monta~ 15 MT 18036 NA 20464 NA ## 9 1 Unive~ 16 NE 13511 NA 18849 NA ## 10 1 Unive~ 15 NV 10551 NA 17582 NA ## # ... with 256 more rows, and 3 more variables: avgfacsal &lt;int&gt;, ## # unemp_rate &lt;lgl&gt;, year &lt;dbl&gt; F_2017 %&gt;% mutate(year = 2017) ## # A tibble: 266 x 11 ## control instnm ccbasic stabbr npt4_pub npt4_priv costt4_a grad_debt_mdn ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 India~ 16 IN 12021 NA 20352 22343 ## 2 1 Unive~ 15 KS 18814 NA 24824 21105 ## 3 2 Bosto~ 15 MA NA 34550 68039 16939 ## 4 1 Oakla~ 16 MI 12779 NA 21262 25000 ## 5 1 Unive~ 15 MS 13929 NA 23606 19500 ## 6 1 Misso~ 16 MO 14473 NA 22045 24750 ## 7 2 Washi~ 15 MO NA 28540 69754 16075 ## 8 1 Monta~ 15 MT 17754 NA 19980 23500 ## 9 1 Unive~ 16 NE 13173 NA 17935 20250 ## 10 1 Unive~ 15 NV 10555 NA 16964 18750 ## # ... with 256 more rows, and 3 more variables: avgfacsal &lt;int&gt;, ## # unemp_rate &lt;lgl&gt;, year &lt;dbl&gt; F_2016 %&gt;% mutate(year = 2016) ## # A tibble: 266 x 11 ## control instnm ccbasic stabbr npt4_pub npt4_priv costt4_a grad_debt_mdn ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 India~ 16 IN 12417 NA 20451 22668. ## 2 1 Unive~ 15 KS 18234 NA 23687 20500 ## 3 2 Bosto~ 15 MA NA 34356 65595 19000 ## 4 1 Oakla~ 16 MI 12415 NA 20502 25000 ## 5 1 Unive~ 15 MS 14494 NA 23372 19500 ## 6 1 Misso~ 16 MO 14303 NA 21698 24250 ## 7 2 Washi~ 15 MO NA 29957 67751 19500 ## 8 1 Monta~ 15 MT 17050 NA 19441 23505 ## 9 1 Unive~ 16 NE 12899 NA 17712 19500 ## 10 1 Unive~ 15 NV 10726 NA 17131 18500 ## # ... with 256 more rows, and 3 more variables: avgfacsal &lt;int&gt;, ## # unemp_rate &lt;lgl&gt;, year &lt;dbl&gt; F_2015 %&gt;% mutate(year = 2015) ## # A tibble: 266 x 11 ## control instnm ccbasic stabbr npt4_pub npt4_priv costt4_a grad_debt_mdn ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 India~ 16 IN 12861 NA 20257 23000 ## 2 1 Unive~ 15 KS 18755 NA 23733 20454 ## 3 2 Bosto~ 15 MA NA 33661 62968 19000 ## 4 1 Oakla~ 16 MI 11929 NA 19475 25000 ## 5 1 Unive~ 15 MS 14284 NA 22704 19500 ## 6 1 Misso~ 16 MO 14096 NA 21793 24266. ## 7 2 Washi~ 15 MO NA 32873 65887 19500 ## 8 1 Monta~ 15 MT 16809 NA 18961 23683 ## 9 1 Unive~ 16 NE 12505 NA 17148 19032 ## 10 1 Unive~ 15 NV 10915 NA 16791 18750 ## # ... with 256 more rows, and 3 more variables: avgfacsal &lt;int&gt;, ## # unemp_rate &lt;lgl&gt;, year &lt;dbl&gt; Finance &lt;- rbind(F_latest, F_2018, F_2017, F_2016, F_2015) write.csv(Finance, &quot;Proj1Data/Financials15to19.csv&quot;) 2.2.2 Demographics D_latest &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr,female, first_gen, poverty_rate, veteran, unemp_rate, ugds_white, ugds_black, ugds_hisp, ugds_asian, ugds_aian, ugds_nhpi, ugds_2mor, ugds_nra, ugds_unkn, ugds_whitenh, ugds_blacknh, ugds_api, ugds_aianold, ugds_hispold) %&gt;% sc_year(&quot;latest&quot;) %&gt;% sc_get() D_2018 &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr, female, first_gen, poverty_rate, veteran, unemp_rate, ugds_white, ugds_black, ugds_hisp, ugds_asian, ugds_aian, ugds_nhpi, ugds_2mor, ugds_nra, ugds_unkn, ugds_whitenh, ugds_blacknh, ugds_api, ugds_aianold, ugds_hispold) %&gt;% sc_year(2018) %&gt;% sc_get() D_2017 &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr, female, first_gen, poverty_rate, veteran, unemp_rate, ugds_white, ugds_black, ugds_hisp, ugds_asian, ugds_aian, ugds_nhpi, ugds_2mor, ugds_nra, ugds_unkn, ugds_whitenh, ugds_blacknh, ugds_api, ugds_aianold, ugds_hispold) %&gt;% sc_year(2017) %&gt;% sc_get() D_2016 &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr, female, first_gen, poverty_rate, veteran, unemp_rate, ugds_white, ugds_black, ugds_hisp, ugds_asian, ugds_aian, ugds_nhpi, ugds_2mor, ugds_nra, ugds_unkn, ugds_whitenh, ugds_blacknh, ugds_api, ugds_aianold, ugds_hispold) %&gt;% sc_year(2016) %&gt;% sc_get() D_2015 &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr, female, first_gen, poverty_rate, veteran, unemp_rate, ugds_white, ugds_black, ugds_hisp, ugds_asian, ugds_aian, ugds_nhpi, ugds_2mor, ugds_nra, ugds_unkn, ugds_whitenh, ugds_blacknh, ugds_api, ugds_aianold, ugds_hispold) %&gt;% sc_year(2015) %&gt;% sc_get() D_latest %&gt;% mutate(year = &quot;latest&quot;) ## # A tibble: 266 x 24 ## control instnm ccbasic stabbr female first_gen poverty_rate veteran ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 India~ 16 IN 0.610 0.351 6.20 0.00394 ## 2 1 Unive~ 15 KS 0.548 0.222 5.24 0.00429 ## 3 2 Bosto~ 15 MA 0.519 0.132 5.93 NA ## 4 1 Oakla~ 16 MI 0.608 0.324 5.74 0.00231 ## 5 1 Unive~ 15 MS 0.608 0.235 12.2 0.00176 ## 6 1 Misso~ 16 MO 0.241 0.225 7.31 NA ## 7 2 Washi~ 15 MO 0.534 0.114 6.17 NA ## 8 1 Monta~ 15 MT 0.493 0.233 8.58 0.00493 ## 9 1 Unive~ 16 NE 0.561 0.348 5.36 0.00304 ## 10 1 Unive~ 15 NV 0.597 0.423 7.25 0.00521 ## # ... with 256 more rows, and 16 more variables: unemp_rate &lt;dbl&gt;, ## # ugds_white &lt;dbl&gt;, ugds_black &lt;dbl&gt;, ugds_hisp &lt;dbl&gt;, ugds_asian &lt;dbl&gt;, ## # ugds_aian &lt;dbl&gt;, ugds_nhpi &lt;dbl&gt;, ugds_2mor &lt;dbl&gt;, ugds_nra &lt;dbl&gt;, ## # ugds_unkn &lt;dbl&gt;, ugds_whitenh &lt;lgl&gt;, ugds_blacknh &lt;lgl&gt;, ugds_api &lt;lgl&gt;, ## # ugds_aianold &lt;lgl&gt;, ugds_hispold &lt;lgl&gt;, year &lt;chr&gt; D_2018 %&gt;% mutate(year = 2018) ## # A tibble: 266 x 24 ## control instnm ccbasic stabbr female first_gen poverty_rate veteran ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1 India~ 16 IN NA NA NA NA ## 2 1 Unive~ 15 KS NA NA NA NA ## 3 2 Bosto~ 15 MA NA NA NA NA ## 4 1 Oakla~ 16 MI NA NA NA NA ## 5 1 Unive~ 15 MS NA NA NA NA ## 6 1 Misso~ 16 MO NA NA NA NA ## 7 2 Washi~ 15 MO NA NA NA NA ## 8 1 Monta~ 15 MT NA NA NA NA ## 9 1 Unive~ 16 NE NA NA NA NA ## 10 1 Unive~ 15 NV NA NA NA NA ## # ... with 256 more rows, and 16 more variables: unemp_rate &lt;lgl&gt;, ## # ugds_white &lt;dbl&gt;, ugds_black &lt;dbl&gt;, ugds_hisp &lt;dbl&gt;, ugds_asian &lt;dbl&gt;, ## # ugds_aian &lt;dbl&gt;, ugds_nhpi &lt;dbl&gt;, ugds_2mor &lt;dbl&gt;, ugds_nra &lt;dbl&gt;, ## # ugds_unkn &lt;dbl&gt;, ugds_whitenh &lt;lgl&gt;, ugds_blacknh &lt;lgl&gt;, ugds_api &lt;lgl&gt;, ## # ugds_aianold &lt;lgl&gt;, ugds_hispold &lt;lgl&gt;, year &lt;dbl&gt; D_2017 %&gt;% mutate(year = 2017) ## # A tibble: 266 x 24 ## control instnm ccbasic stabbr female first_gen poverty_rate veteran ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1 India~ 16 IN NA NA NA NA ## 2 1 Unive~ 15 KS NA NA NA NA ## 3 2 Bosto~ 15 MA NA NA NA NA ## 4 1 Oakla~ 16 MI NA NA NA NA ## 5 1 Unive~ 15 MS NA NA NA NA ## 6 1 Misso~ 16 MO NA NA NA NA ## 7 2 Washi~ 15 MO NA NA NA NA ## 8 1 Monta~ 15 MT NA NA NA NA ## 9 1 Unive~ 16 NE NA NA NA NA ## 10 1 Unive~ 15 NV NA NA NA NA ## # ... with 256 more rows, and 16 more variables: unemp_rate &lt;lgl&gt;, ## # ugds_white &lt;dbl&gt;, ugds_black &lt;dbl&gt;, ugds_hisp &lt;dbl&gt;, ugds_asian &lt;dbl&gt;, ## # ugds_aian &lt;dbl&gt;, ugds_nhpi &lt;dbl&gt;, ugds_2mor &lt;dbl&gt;, ugds_nra &lt;dbl&gt;, ## # ugds_unkn &lt;dbl&gt;, ugds_whitenh &lt;lgl&gt;, ugds_blacknh &lt;lgl&gt;, ugds_api &lt;lgl&gt;, ## # ugds_aianold &lt;lgl&gt;, ugds_hispold &lt;lgl&gt;, year &lt;dbl&gt; D_2016 %&gt;% mutate(year = 2016) ## # A tibble: 266 x 24 ## control instnm ccbasic stabbr female first_gen poverty_rate veteran ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 1 India~ 16 IN 0.610 0.351 NA 0.00394 ## 2 1 Unive~ 15 KS 0.548 0.222 NA 0.00429 ## 3 2 Bosto~ 15 MA 0.519 0.132 NA NA ## 4 1 Oakla~ 16 MI 0.608 0.324 NA 0.00231 ## 5 1 Unive~ 15 MS 0.608 0.235 NA 0.00176 ## 6 1 Misso~ 16 MO 0.241 0.225 NA NA ## 7 2 Washi~ 15 MO 0.534 0.114 NA NA ## 8 1 Monta~ 15 MT 0.493 0.233 NA 0.00493 ## 9 1 Unive~ 16 NE 0.561 0.348 NA 0.00304 ## 10 1 Unive~ 15 NV 0.597 0.423 NA 0.00521 ## # ... with 256 more rows, and 16 more variables: unemp_rate &lt;lgl&gt;, ## # ugds_white &lt;dbl&gt;, ugds_black &lt;dbl&gt;, ugds_hisp &lt;dbl&gt;, ugds_asian &lt;dbl&gt;, ## # ugds_aian &lt;dbl&gt;, ugds_nhpi &lt;dbl&gt;, ugds_2mor &lt;dbl&gt;, ugds_nra &lt;dbl&gt;, ## # ugds_unkn &lt;dbl&gt;, ugds_whitenh &lt;lgl&gt;, ugds_blacknh &lt;lgl&gt;, ugds_api &lt;lgl&gt;, ## # ugds_aianold &lt;lgl&gt;, ugds_hispold &lt;lgl&gt;, year &lt;dbl&gt; D_2015 %&gt;% mutate(year = 2015) ## # A tibble: 266 x 24 ## control instnm ccbasic stabbr female first_gen poverty_rate veteran ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 1 India~ 16 IN 0.607 0.361 NA 0.00390 ## 2 1 Unive~ 15 KS 0.536 0.226 NA 0.00534 ## 3 2 Bosto~ 15 MA 0.532 0.125 NA NA ## 4 1 Oakla~ 16 MI 0.599 0.325 NA 0.00207 ## 5 1 Unive~ 15 MS 0.613 0.226 NA 0.00209 ## 6 1 Misso~ 16 MO 0.228 0.226 NA NA ## 7 2 Washi~ 15 MO 0.541 0.0897 NA NA ## 8 1 Monta~ 15 MT 0.492 0.231 NA 0.00589 ## 9 1 Unive~ 16 NE 0.55 0.337 NA 0.00492 ## 10 1 Unive~ 15 NV 0.586 0.419 NA 0.00573 ## # ... with 256 more rows, and 16 more variables: unemp_rate &lt;lgl&gt;, ## # ugds_white &lt;dbl&gt;, ugds_black &lt;dbl&gt;, ugds_hisp &lt;dbl&gt;, ugds_asian &lt;dbl&gt;, ## # ugds_aian &lt;dbl&gt;, ugds_nhpi &lt;dbl&gt;, ugds_2mor &lt;dbl&gt;, ugds_nra &lt;dbl&gt;, ## # ugds_unkn &lt;dbl&gt;, ugds_whitenh &lt;lgl&gt;, ugds_blacknh &lt;lgl&gt;, ugds_api &lt;lgl&gt;, ## # ugds_aianold &lt;lgl&gt;, ugds_hispold &lt;lgl&gt;, year &lt;dbl&gt; Demographics &lt;- rbind(D_latest, D_2018, D_2017, D_2016, D_2015) write.csv(Demographics, &quot;Proj1Data/Demographics15to19.csv&quot;) 2.2.3 Death Dth_latest &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr, death_yr2_rt, lo_inc_death_yr2_rt, md_inc_death_yr2_rt, hi_inc_death_yr2_rt, death_yr3_rt, lo_inc_death_yr3_rt, md_inc_death_yr3_rt, hi_inc_death_yr3_rt, death_yr4_rt, lo_inc_death_yr4_rt, md_inc_death_yr4_rt, hi_inc_death_yr4_rt, death_yr6_rt, lo_inc_death_yr6_rt, md_inc_death_yr6_rt, hi_inc_death_yr6_rt, death_yr8_rt, lo_inc_death_yr8_rt, md_inc_death_yr8_rt, hi_inc_death_yr8_rt) %&gt;% sc_year(&quot;latest&quot;) %&gt;% sc_get() Dth_2018 &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr,death_yr2_rt, lo_inc_death_yr2_rt, md_inc_death_yr2_rt, hi_inc_death_yr2_rt, death_yr3_rt, lo_inc_death_yr3_rt, md_inc_death_yr3_rt, hi_inc_death_yr3_rt, death_yr4_rt, lo_inc_death_yr4_rt, md_inc_death_yr4_rt, hi_inc_death_yr4_rt, death_yr6_rt, lo_inc_death_yr6_rt, md_inc_death_yr6_rt, hi_inc_death_yr6_rt, death_yr8_rt, lo_inc_death_yr8_rt, md_inc_death_yr8_rt, hi_inc_death_yr8_rt) %&gt;% sc_year(2018) %&gt;% sc_get() Dth_2017 &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr, death_yr2_rt, lo_inc_death_yr2_rt, md_inc_death_yr2_rt, hi_inc_death_yr2_rt, death_yr3_rt, lo_inc_death_yr3_rt, md_inc_death_yr3_rt, hi_inc_death_yr3_rt, death_yr4_rt, lo_inc_death_yr4_rt, md_inc_death_yr4_rt, hi_inc_death_yr4_rt, death_yr6_rt, lo_inc_death_yr6_rt, md_inc_death_yr6_rt, hi_inc_death_yr6_rt, death_yr8_rt, lo_inc_death_yr8_rt, md_inc_death_yr8_rt, hi_inc_death_yr8_rt) %&gt;% sc_year(2017) %&gt;% sc_get() Dth_2016 &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr, death_yr2_rt, lo_inc_death_yr2_rt, md_inc_death_yr2_rt, hi_inc_death_yr2_rt, death_yr3_rt, lo_inc_death_yr3_rt, md_inc_death_yr3_rt, hi_inc_death_yr3_rt, death_yr4_rt, lo_inc_death_yr4_rt, md_inc_death_yr4_rt, hi_inc_death_yr4_rt, death_yr6_rt, lo_inc_death_yr6_rt, md_inc_death_yr6_rt, hi_inc_death_yr6_rt, death_yr8_rt, lo_inc_death_yr8_rt, md_inc_death_yr8_rt, hi_inc_death_yr8_rt) %&gt;% sc_year(2016) %&gt;% sc_get() Dth_2015 &lt;- sc_init() %&gt;% sc_filter(distanceonly == 0, ccbasic %in% c(15, 16)) %&gt;% sc_select(control, instnm, ccbasic, stabbr, death_yr2_rt, lo_inc_death_yr2_rt, md_inc_death_yr2_rt, hi_inc_death_yr2_rt, death_yr3_rt, lo_inc_death_yr3_rt, md_inc_death_yr3_rt, hi_inc_death_yr3_rt, death_yr4_rt, lo_inc_death_yr4_rt, md_inc_death_yr4_rt, hi_inc_death_yr4_rt, death_yr6_rt, lo_inc_death_yr6_rt, md_inc_death_yr6_rt, hi_inc_death_yr6_rt, death_yr8_rt, lo_inc_death_yr8_rt, md_inc_death_yr8_rt, hi_inc_death_yr8_rt) %&gt;% sc_year(2015) %&gt;% sc_get() Dth_latest %&gt;% mutate(year = &quot;latest&quot;) ## # A tibble: 266 x 25 ## control instnm ccbasic stabbr death_yr2_rt lo_inc_death_yr~ md_inc_death_yr~ ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1 India~ 16 IN NA NA NA ## 2 1 Unive~ 15 KS NA NA NA ## 3 2 Bosto~ 15 MA NA NA NA ## 4 1 Oakla~ 16 MI NA NA NA ## 5 1 Unive~ 15 MS NA NA NA ## 6 1 Misso~ 16 MO NA NA NA ## 7 2 Washi~ 15 MO NA NA NA ## 8 1 Monta~ 15 MT NA NA NA ## 9 1 Unive~ 16 NE NA NA NA ## 10 1 Unive~ 15 NV NA NA NA ## # ... with 256 more rows, and 18 more variables: hi_inc_death_yr2_rt &lt;lgl&gt;, ## # death_yr3_rt &lt;dbl&gt;, lo_inc_death_yr3_rt &lt;lgl&gt;, md_inc_death_yr3_rt &lt;lgl&gt;, ## # hi_inc_death_yr3_rt &lt;lgl&gt;, death_yr4_rt &lt;dbl&gt;, lo_inc_death_yr4_rt &lt;dbl&gt;, ## # md_inc_death_yr4_rt &lt;dbl&gt;, hi_inc_death_yr4_rt &lt;lgl&gt;, death_yr6_rt &lt;dbl&gt;, ## # lo_inc_death_yr6_rt &lt;dbl&gt;, md_inc_death_yr6_rt &lt;dbl&gt;, ## # hi_inc_death_yr6_rt &lt;lgl&gt;, death_yr8_rt &lt;dbl&gt;, lo_inc_death_yr8_rt &lt;dbl&gt;, ## # md_inc_death_yr8_rt &lt;dbl&gt;, hi_inc_death_yr8_rt &lt;lgl&gt;, year &lt;chr&gt; Dth_2018 %&gt;% mutate(year = 2018) ## # A tibble: 266 x 25 ## control instnm ccbasic stabbr death_yr2_rt lo_inc_death_yr~ md_inc_death_yr~ ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1 India~ 16 IN NA NA NA ## 2 1 Unive~ 15 KS NA NA NA ## 3 2 Bosto~ 15 MA NA NA NA ## 4 1 Oakla~ 16 MI NA NA NA ## 5 1 Unive~ 15 MS NA NA NA ## 6 1 Misso~ 16 MO NA NA NA ## 7 2 Washi~ 15 MO NA NA NA ## 8 1 Monta~ 15 MT NA NA NA ## 9 1 Unive~ 16 NE NA NA NA ## 10 1 Unive~ 15 NV NA NA NA ## # ... with 256 more rows, and 18 more variables: hi_inc_death_yr2_rt &lt;lgl&gt;, ## # death_yr3_rt &lt;lgl&gt;, lo_inc_death_yr3_rt &lt;lgl&gt;, md_inc_death_yr3_rt &lt;lgl&gt;, ## # hi_inc_death_yr3_rt &lt;lgl&gt;, death_yr4_rt &lt;lgl&gt;, lo_inc_death_yr4_rt &lt;lgl&gt;, ## # md_inc_death_yr4_rt &lt;lgl&gt;, hi_inc_death_yr4_rt &lt;lgl&gt;, death_yr6_rt &lt;lgl&gt;, ## # lo_inc_death_yr6_rt &lt;lgl&gt;, md_inc_death_yr6_rt &lt;lgl&gt;, ## # hi_inc_death_yr6_rt &lt;lgl&gt;, death_yr8_rt &lt;lgl&gt;, lo_inc_death_yr8_rt &lt;lgl&gt;, ## # md_inc_death_yr8_rt &lt;lgl&gt;, hi_inc_death_yr8_rt &lt;lgl&gt;, year &lt;dbl&gt; Dth_2017 %&gt;% mutate(year = 2017) ## # A tibble: 266 x 25 ## control instnm ccbasic stabbr death_yr2_rt lo_inc_death_yr~ md_inc_death_yr~ ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1 India~ 16 IN NA NA NA ## 2 1 Unive~ 15 KS NA NA NA ## 3 2 Bosto~ 15 MA NA NA NA ## 4 1 Oakla~ 16 MI NA NA NA ## 5 1 Unive~ 15 MS NA NA NA ## 6 1 Misso~ 16 MO NA NA NA ## 7 2 Washi~ 15 MO NA NA NA ## 8 1 Monta~ 15 MT NA NA NA ## 9 1 Unive~ 16 NE NA NA NA ## 10 1 Unive~ 15 NV NA NA NA ## # ... with 256 more rows, and 18 more variables: hi_inc_death_yr2_rt &lt;lgl&gt;, ## # death_yr3_rt &lt;lgl&gt;, lo_inc_death_yr3_rt &lt;lgl&gt;, md_inc_death_yr3_rt &lt;lgl&gt;, ## # hi_inc_death_yr3_rt &lt;lgl&gt;, death_yr4_rt &lt;lgl&gt;, lo_inc_death_yr4_rt &lt;lgl&gt;, ## # md_inc_death_yr4_rt &lt;lgl&gt;, hi_inc_death_yr4_rt &lt;lgl&gt;, death_yr6_rt &lt;lgl&gt;, ## # lo_inc_death_yr6_rt &lt;lgl&gt;, md_inc_death_yr6_rt &lt;lgl&gt;, ## # hi_inc_death_yr6_rt &lt;lgl&gt;, death_yr8_rt &lt;lgl&gt;, lo_inc_death_yr8_rt &lt;lgl&gt;, ## # md_inc_death_yr8_rt &lt;lgl&gt;, hi_inc_death_yr8_rt &lt;lgl&gt;, year &lt;dbl&gt; Dth_2016 %&gt;% mutate(year = 2016) ## # A tibble: 266 x 25 ## control instnm ccbasic stabbr death_yr2_rt lo_inc_death_yr~ md_inc_death_yr~ ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1 India~ 16 IN NA NA NA ## 2 1 Unive~ 15 KS NA NA NA ## 3 2 Bosto~ 15 MA NA NA NA ## 4 1 Oakla~ 16 MI NA NA NA ## 5 1 Unive~ 15 MS NA NA NA ## 6 1 Misso~ 16 MO NA NA NA ## 7 2 Washi~ 15 MO NA NA NA ## 8 1 Monta~ 15 MT NA NA NA ## 9 1 Unive~ 16 NE NA NA NA ## 10 1 Unive~ 15 NV NA NA NA ## # ... with 256 more rows, and 18 more variables: hi_inc_death_yr2_rt &lt;lgl&gt;, ## # death_yr3_rt &lt;dbl&gt;, lo_inc_death_yr3_rt &lt;lgl&gt;, md_inc_death_yr3_rt &lt;lgl&gt;, ## # hi_inc_death_yr3_rt &lt;lgl&gt;, death_yr4_rt &lt;dbl&gt;, lo_inc_death_yr4_rt &lt;dbl&gt;, ## # md_inc_death_yr4_rt &lt;dbl&gt;, hi_inc_death_yr4_rt &lt;lgl&gt;, death_yr6_rt &lt;dbl&gt;, ## # lo_inc_death_yr6_rt &lt;dbl&gt;, md_inc_death_yr6_rt &lt;dbl&gt;, ## # hi_inc_death_yr6_rt &lt;lgl&gt;, death_yr8_rt &lt;dbl&gt;, lo_inc_death_yr8_rt &lt;dbl&gt;, ## # md_inc_death_yr8_rt &lt;dbl&gt;, hi_inc_death_yr8_rt &lt;lgl&gt;, year &lt;dbl&gt; Dth_2015 %&gt;% mutate(year = 2015) ## # A tibble: 266 x 25 ## control instnm ccbasic stabbr death_yr2_rt lo_inc_death_yr~ md_inc_death_yr~ ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1 India~ 16 IN NA NA NA ## 2 1 Unive~ 15 KS NA NA NA ## 3 2 Bosto~ 15 MA NA NA NA ## 4 1 Oakla~ 16 MI NA NA NA ## 5 1 Unive~ 15 MS NA NA NA ## 6 1 Misso~ 16 MO NA NA NA ## 7 2 Washi~ 15 MO NA NA NA ## 8 1 Monta~ 15 MT NA NA NA ## 9 1 Unive~ 16 NE NA NA NA ## 10 1 Unive~ 15 NV NA NA NA ## # ... with 256 more rows, and 18 more variables: hi_inc_death_yr2_rt &lt;lgl&gt;, ## # death_yr3_rt &lt;dbl&gt;, lo_inc_death_yr3_rt &lt;lgl&gt;, md_inc_death_yr3_rt &lt;lgl&gt;, ## # hi_inc_death_yr3_rt &lt;lgl&gt;, death_yr4_rt &lt;dbl&gt;, lo_inc_death_yr4_rt &lt;lgl&gt;, ## # md_inc_death_yr4_rt &lt;lgl&gt;, hi_inc_death_yr4_rt &lt;lgl&gt;, death_yr6_rt &lt;dbl&gt;, ## # lo_inc_death_yr6_rt &lt;dbl&gt;, md_inc_death_yr6_rt &lt;lgl&gt;, ## # hi_inc_death_yr6_rt &lt;lgl&gt;, death_yr8_rt &lt;dbl&gt;, lo_inc_death_yr8_rt &lt;dbl&gt;, ## # md_inc_death_yr8_rt &lt;lgl&gt;, hi_inc_death_yr8_rt &lt;lgl&gt;, year &lt;dbl&gt; Death &lt;- rbind(Dth_latest, Dth_2018, Dth_2017, Dth_2016, Dth_2015) write.csv(Death, &quot;Proj1Data/Death15to19.csv&quot;) R1Schools &lt;- sc_init() %&gt;% sc_filter(ccbasic == 15) %&gt;% sc_select(control, instnm, stabbr) %&gt;% sc_year(&quot;latest&quot;) %&gt;% sc_get() "],["quality.html", "Section 3 Data Quality 3.1 Set-up 3.2 The Functions 3.3 The Evaluations", " Section 3 Data Quality 3.1 Set-up First, I import the packages that I use in this section and read in the uncleaned data. #LIBRARIES library(tidyverse) #Reading in the data fin &lt;- read.csv(&#39;Proj1Data/Financials15to19.csv&#39;) fin &lt;- fin[,-1] dem &lt;- read.csv(&quot;Proj1Data/Demographics15to19.csv&quot;) dem &lt;- dem[,-1] 3.2 The Functions CONSISTANT REPRESENTATION con_rep &lt;- function(df){ &quot; A function that quantitatively scores an input data frame on the consistancy of representation data quality metric. Input: df: a data frame Output: con_rep_score: A numeric score on consistency of representation ranging from 1 to 0, where 1 is perfectly consistent representation and 0 is inconsistent representation. &quot; type = vector() for(i in 1:ncol(df)){ col_type &lt;- typeof(df[1,i]) type[i] &lt;- col_type } con_rep_score &lt;- 1 - ((length(unique(type)) - 1)/6) return(con_rep_score) } COMPLETENESS AND EASE OF MANIPULATION data_quality &lt;- function(df){ &quot; A function to quantitatively compute scores for a dataframe on the completeness and ease of manipulation data quality metrics. Input: df: A data frame Output: qualityTable: A table reporting the scores on completeness and ease of manipulation for each column in the input data frame. &quot; # Setting the index value, which will be used to index the column name index &lt;- 1 # Instantiating empty data frames for each of the queries completeness &lt;- data.frame(Completeness=double()) eom &lt;- data.frame(Ease_of_Manipulation=double()) names &lt;- data.frame(ColumnName=character()) # Populating the data frames using a for-loop for (i in df){ # COLLECTING THE NAMES OF EACH COLUMN PASSED col &lt;- colnames(df[index]) # COMPLETENESS # Takes the sum of the total NA, NULL, and NaN values in a column # Divides them by the length of the column # Subtracts this from one, as was suggested by Pipinio, Lee, and Wang # And then rounds to output to the third decimal place c &lt;- 1-(sum(is.na(i) + is.null(i) + is.nan(i))/length(i)) %&gt;% round(digits = 3) # EASE OF MANIPULATION # &quot;Case when&quot; vectorises a series of if/else statements # The function checks the type of the column and then sets the variable, # e, to the corresponding value. e &lt;- case_when( typeof(i) == &quot;logical&quot; ~ 1, typeof(i) == &quot;integer&quot; ~ .9, typeof(i) == &quot;numeric&quot; ~ .8, typeof(i) == &quot;double&quot; ~ .8, typeof(i) == &quot;complex&quot; ~ .7, typeof(i) == &quot;character&quot; ~ .6, typeof(i) == &quot;list&quot; ~ .5, typeof(i) == &quot;raw&quot; ~ 0, TRUE ~ 0) #The index used to collect column names is increased by one index = index + 1 #Appending the output for each column to their respective data frames completeness[nrow(completeness)+1,] &lt;- c eom[nrow(eom)+1,] &lt;- e names[nrow(names)+1,] &lt;- col } #Binding the columns of the three tables into an output table qualityTable &lt;- cbind(names, completeness, eom) return(qualityTable) } 3.3 The Evaluations We assessed data quality using the metrics outlined in (paper). For each of these metrics, we provide a brief commentary on how the data fared. Several of the data quality metrics were more pertinent to our analysis, so we provide deeper insight into them. Accessibility On its own, the data were not accessible and extraction proved to be exceedingly challenging. By using the {rscorecard} package, we were able to streamline the process. However, {rscorecard} is not built into the API and cannot be factored into the datasets baseline accessibility. Despite the challenge of pulling the data into an interpretable form, they can be accessed by the public. This is to say, we did not have to fight with a Facebook executive to access them. We scored the data at a 4/10 for accessibility. Believability There is no reason for us to assume the data would be intentionally falsified as it was collected by the U.S. Government to simply compare costs and values of higher education institutions. There have been claims from some colleges (such as Boston University) that there are inaccuracies in the data, however. We decided to give the data a score of 8/10 on believability. Concise Representation The data is difficult to navigate through without frequent cross-checking of the documentation. There are many columns that look quite similar but are entirely different measures and statistics. For example, within the demographics data there are multiple ways the racial makeup of a university are encoded. These columns appear to contain the same information drawn from perhaps different sources. However, the data is not consistent between sources and, at least within the demographics variables, contained large swaths of NAs. Based on our subjective experience with the data, we assigned it a score of 4/10 because it could, theoretically, be worse. Consistent Representation We calculated consistent representation by creating a function that collected the type for each row of a data frame. The function counts the number of unique types that occur in the data frame minus one (for scoring purposes). This number is then divided by the total number of types possible minus one. This number is subtracted from 1. If all of the data are encoded the same way, they will receive a score of 1. This is because the function will read that there is one unique type in the data frame. One will be subtracted from this, leading the function to divide 0/6, resulting in 0. 1 - 0 is 1. 1 was set as the highest score so that it would correspond with the other measures of data quality. If the data uses all 7 different kinds of data type that can be reasonably encoded, it will receive a score of 0. The Financials data set had three unique types, and received a score of .667. The Demographics data set had four unique data types, and received a score of 0.5. #CONSISTENCY OF REPRESENTATION FOR THE FINANCIALS DATA paste(&quot;Score:&quot;, round(con_rep(fin),3)) ## [1] &quot;Score: 0.667&quot; #CONSISTENCY OF REPRESENTATION FOR THE DEMOGRAPHICS DATA paste(&quot;Score:&quot;, round(con_rep(dem),3)) ## [1] &quot;Score: 0.5&quot; Completeness Completeness was measured by summing the total number of blank data rows in each column - blank referring to rows containing either NA, NaN, or Null - and dividing this by the total number of rows. A score of 1 indicates perfect completeness. A score of 0 indicates no completeness, or a column of entirely null data. Within the Financial dataset, scores on completeness varied from 1, for the key values, to .195 for the unemployment rate. The mean completeness score over the entire dataset was 0.812, which indicates a moderate level of completeness. The Demographic dataset had a wider range of scores. Like the Financial dataset, the key values were awarded a completeness score of 1. However, response rates for the demographic variables had a wide distribution. The major races, such as White, Black, Hispanic, Asian, etc. had response rates of .983. However, response rates for ethnicities, such as White (non-hispanic) and Black (non-hispanic) had a response rate of 0. These variables were dropped during the cleaning process; however, they still skewed the mean completeness score for this variable. The mean completeness for the Demographic dataset was 0.658. Ease of Manipulation Ease of manipulation was quantified using the rules of coercion in R (as outlined in OReilly, R in a Nutshell, 2nd Edition). Since certain base R operations can only be performed on specific data types, the data type integrally influences how easily the data can be used and manipulated. The easier a data type is to coerce, or manipulate, into another data type, the higher a score it obtains. The gradient of scores moves from the logical (i.e., TRUE/FALSE) data type, which can be easily coerced into every other data types, to the raw data type, which cannot be implicitly coerced into other data types and is difficult to explicitly coerce. Because the logical data type can be universally manipulated, it scores a one. Since the raw data type cannot be coerced into another data type, it obtains an ease of manipulation score of zero. Rare data types, such as time series, were not included in the base version of this function because they are rarely included in data frames. Matrices were not included because it is nearly impossible to work them into a data frame, and because typeof returns their type as the type of data that is included within them. The ease of manipulation scores for both data sets are included below. The average ease of manipulation score for the Financials data set was 0.8, which indicates good ease of manipulation. The average ease of manipulation score for the Demographics data set was 0.825, which also indicates good ease of manipulation. ## ColumnName Completeness Ease_of_Manipulation ## 1 control 1.000 0.9 ## 2 instnm 1.000 0.6 ## 3 ccbasic 1.000 0.9 ## 4 stabbr 1.000 0.6 ## 5 npt4_pub 0.688 0.9 ## 6 npt4_priv 0.287 0.9 ## 7 costt4_a 0.977 0.9 ## 8 grad_debt_mdn 0.785 0.8 ## 9 avgfacsal 0.996 0.9 ## 10 unemp_rate 0.195 0.8 ## 11 year 1.000 0.6 ## [1] &quot;Mean over columns: 0.8&quot; ## ColumnName Completeness Ease_of_Manipulation ## 1 control 1.000 0.9 ## 2 instnm 1.000 0.6 ## 3 ccbasic 1.000 0.9 ## 4 stabbr 1.000 0.6 ## 5 female 0.591 0.8 ## 6 first_gen 0.586 0.8 ## 7 poverty_rate 0.195 0.8 ## 8 veteran 0.382 0.8 ## 9 unemp_rate 0.195 0.8 ## 10 ugds_white 0.983 0.8 ## 11 ugds_black 0.983 0.8 ## 12 ugds_hisp 0.983 0.8 ## 13 ugds_asian 0.983 0.8 ## 14 ugds_aian 0.983 0.8 ## 15 ugds_nhpi 0.983 0.8 ## 16 ugds_2mor 0.983 0.8 ## 17 ugds_nra 0.983 0.8 ## 18 ugds_unkn 0.983 0.8 ## 19 ugds_whitenh 0.000 1.0 ## 20 ugds_blacknh 0.000 1.0 ## 21 ugds_api 0.000 1.0 ## 22 ugds_aianold 0.000 1.0 ## 23 ugds_hispold 0.000 1.0 ## 24 year 1.000 0.6 ## [1] &quot;Mean over columns: 0.825&quot; Reputation The data is well regarded as it has been used for other studies and was introduced by Obama in 2015. It highlighted the Pell grant problem and other financial issues being faced by college students across the country. Based on our subjective experience with the data, we assigned it a score of 8/10. It was not assigned a 10/10 because of some disputes about the quality of the data which have been raised by universities included in it. Security The data is highly secured as it was contained and dispersed by the United States Government. It is publicly accessible - however, this should not compromise the security of the data. There have been no known hackings of the data. We decided to assign the data a security score of 10/10. Timeliness The data is mostly up to date, but it would have been nice to have more recent statistics being recorded as financial data can change quickly as a product of circumstances; like a pandemic. Based on our subjective experience with the data, we assigned it a score of 9/10. "],["clean.html", "Section 4 Data Cleaning 4.1 Demographics", " Section 4 Data Cleaning After the initial parsing of the data in the acquisition phase, there was not a significant amount of cleaning to be done. The cleaning stage primarily consisted of organizing the variables in a way that was intuitive to work with. We chose to handle NAs by replacing them with the group mean determined by their Carnegie classification. For the demographics data set, we first dropped extraneous columns. These were columns which consisted of more than 90% NA values. Then, we renamed the variables so that they would be more intuitive to a general audience (and so that we could remember them better). There were four schools within the data that did not report any demographic variables for every year that they were in the data set. These schools were removed using drop_na(black), because the black column did not possess any NAs other than for these four schools. For both data sets, we chose to recode the Carnegie Classification variable (called r_status) as R1 and R2 to increase the interpretability of the data. These are converted to factors in the analysis section so that they can be input into the models. 4.1 Demographics #IMPORTING PACKAGES library(tidyverse) # READING IN THE CSV dem &lt;- read.csv(&quot;Proj1Data/Demographics15to19.csv&quot;) # CLEANING THE DATA FRAME dem_clean &lt;- dem[,-c(1, 17, 20:24)] %&gt;% rename(&quot;university&quot; = instnm, &quot;r_status&quot; = ccbasic, &quot;state&quot; = stabbr, &quot;white&quot; = ugds_white, &quot;black&quot; = ugds_black, &quot;hispanic&quot; = ugds_hisp, &quot;asian&quot; = ugds_asian, &quot;indigenous&quot; = ugds_aian, &quot;nhpi&quot; = ugds_nhpi, &quot;nra&quot; = ugds_nra, &quot;unknown&quot; = ugds_unkn) %&gt;% drop_na(black) # RECODING THE CARNEGIE CLASSIFICATION (R_STATUS) VARIABLE dem_clean$r_status &lt;- recode(dem_clean$r_status, `15` = &#39;R1&#39;, `16` = &quot;R2&quot;) # NA HANDLING dem_clean &lt;- dem_clean %&gt;% group_by(r_status) %&gt;% mutate_at(c(&quot;female&quot;, &quot;first_gen&quot;, &quot;poverty_rate&quot;, &quot;veteran&quot;, &quot;unemp_rate&quot;), funs(ifelse(is.na(.), mean(., na.rm = TRUE),.))) # WRITING TO A NEW CSV write.csv(dem_clean, &quot;Proj1Data/cleanDemographics1519.csv&quot;) ##Financials The Financials data set was easier to work with; as such, we simply renamed the common variables that we had renamed with demographics and re-coded the Carnegie Classification. # READING IN THE CSV fin &lt;- read.csv(&#39;Proj1Data/Financials15to19.csv&#39;) # RENAMING VARIABLES SO THAT THEY MATCH THE DEMOGRAPHIC DATA FRAME fin_clean &lt;- fin[,-c(2)] %&gt;% rename(&quot;university&quot; = instnm, &quot;r_status&quot; = ccbasic, &quot;state&quot; = stabbr) # RECODING THE CARNEGIE CLASSIFICATION (R_STATUS) VARIABLE fin_clean$r_status &lt;- recode(fin_clean$r_status, `15` = &#39;R1&#39;, `16` = &quot;R2&quot;) # WRITING TO A NEW CSV write.csv(fin_clean, &quot;Proj1Data/cleanFinancials1519.csv&quot;) #DISPLAY OF THE NEW CSV head(dem_clean) %&gt;% MakePretty() control university r_status state female first_gen poverty_rate veteran unemp_rate white black hispanic asian indigenous nhpi nra unknown year 1 Indiana University-Purdue University-Indianapolis R2 IN 0.6099409 0.3508615 6.20 0.0039431 2.90 0.6842 0.0911 0.0823 0.0496 0.0008 4e-04 0.0396 0.0050 latest 1 University of Kansas R1 KS 0.5475972 0.2222402 5.24 0.0042853 2.50 0.7078 0.0425 0.0847 0.0480 0.0031 9e-04 0.0570 0.0042 latest 2 Boston College R1 MA 0.5189448 0.1318957 5.93 0.0037247 2.88 0.5888 0.0408 0.1097 0.1008 0.0002 3e-04 0.0779 0.0472 latest 1 Oakland University R2 MI 0.6078042 0.3242526 5.74 0.0023148 3.21 0.7388 0.0770 0.0389 0.0478 0.0030 1e-03 0.0218 0.0392 latest 1 University of Mississippi R1 MS 0.6080563 0.2350269 12.17 0.0017583 3.64 0.7784 0.1233 0.0355 0.0188 0.0029 1e-03 0.0160 0.0001 latest 1 Missouri University of Science and Technology R2 MO 0.2413202 0.2249213 7.31 0.0049616 3.05 0.8149 0.0302 0.0383 0.0371 0.0029 0e+00 0.0273 0.0162 latest "],["analysis.html", "Section 5 Analysis 5.1 Import Packages 5.2 Transform Datasets 5.3 Descriptives 5.4 Wilcox signed-rank test 5.5 Welchs two-sample t-test 5.6 Regression model 5.7 Model step 1 5.8 Model step 2 5.9 Model step 3 5.10 Complete model", " Section 5 Analysis 5.1 Import Packages library(moments) library(dplyr) library(tidyr) library(stats) library(sjstats) library(ggplot2) fin &lt;- read.csv(&#39;Proj1Data/cleanFinancials1519.csv&#39;) dem &lt;- read.csv(&#39;Proj1Data/cleanDemographics1519.csv&#39;) fin$r_status &lt;- factor(fin$r_status, levels = c(&quot;R1&quot;, &quot;R2&quot;), labels = c(1,2)) dem$r_status &lt;- factor(dem$r_status, levels = c(&quot;R1&quot;, &quot;R2&quot;), labels = c(1,2)) 5.2 Transform Datasets finR &lt;- fin[fin$year == &quot;latest&quot;,] finR1 &lt;- fin[fin$r_status == 1 &amp; fin$year == &quot;latest&quot;,] finR2 &lt;- fin[fin$r_status == 2 &amp; fin$year == &quot;latest&quot;,] demR &lt;- dem[dem$year == &quot;latest&quot;,] demR1 &lt;- dem[dem$r_status == 1 &amp; dem$year == &quot;latest&quot;,] demR2 &lt;- dem[dem$r_status == 2 &amp; dem$year == &quot;latest&quot;,] finR &lt;- subset(finR, university %in% demR$university) 5.3 Descriptives # Demographics summary(demR1$white) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0573 0.3900 0.5072 0.5129 0.6763 0.8408 skewness(demR1$white) ## [1] -0.2433317 kurtosis(demR1$white) ## [1] 2.272319 summary(demR2$white) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.4507 0.6242 0.5529 0.7326 0.9348 skewness(demR2$white) ## [1] -0.9043165 kurtosis(demR2$white) ## [1] 2.917131 # Cost print(&#39;Tuition cost&#39;) ## [1] &quot;Tuition cost&quot; summary(finR1$costt4_a) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 16927 24464 28408 38695 66923 75735 1 cost_hist1 &lt;- ggplot(finR1, aes(x =costt4_a)) + geom_histogram(bins = 40, color = &quot;black&quot;, fill = &quot;red&quot;) + labs(title = &quot;Distribution of Tuition Costs at R1s&quot;, subtitle = &quot;&quot;, x = &quot;Cost (in American dollars)&quot;, y = &quot;Frequency&quot;) cost_hist1 ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 11299 21266 24098 33613 51214 71875 4 # Students debt print(&#39;Student debt&#39;) ## [1] &quot;Student debt&quot; summary(finR1$grad_debt_mdn) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 8700 17634 20000 19652 22027 27000 debt_hist1 &lt;- ggplot(finR1, aes(x = grad_debt_mdn)) + geom_histogram(bins = 40, color = &quot;black&quot;, fill = &quot;cadetblue4&quot;) + labs(title = &quot;Distribution of Median Student Debt at R1s&quot;, subtitle = &quot;&quot;, x = &quot;Median Debt (in American dollars)&quot;, y = &quot;Frequency&quot;) debt_hist1 summary(finR2$grad_debt_mdn) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 5500 20930 23167 22719 25000 30500 5 debt_hist1 &lt;- ggplot(finR2, aes(x = grad_debt_mdn)) + geom_histogram(bins = 40, color = &quot;black&quot;, fill = &quot;cadetblue3&quot;) + labs(title = &quot;Distribution of Median Student Debt at R2s&quot;, subtitle = &quot;&quot;, x = &quot;Median Debt (in American dollars)&quot;, y = &quot;Frequency&quot;) debt_hist1 # Unemployment print(&#39;Unemployment Rate&#39;) ## [1] &quot;Unemployment Rate&quot; summary(finR1$unemp_rate) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.190 2.965 3.200 3.293 3.605 5.020 unemp_hist1 &lt;- ggplot(finR1, aes(x = unemp_rate)) + geom_histogram(bins = 40, color = &quot;black&quot;, fill = &quot;darkseagreen4&quot;) + labs(title = &quot;Distribution of Post-Graduation Unemployment rates at R1s&quot;, subtitle = &quot;&quot;, x = &quot;Unemployment Rate&quot;, y = &quot;Frequency&quot;) unemp_hist1 summary(finR2$unemp_rate) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 2.320 3.040 3.360 3.545 3.900 7.920 6 unemp_hist2 &lt;- ggplot(finR2, aes(x = unemp_rate)) + geom_histogram(bins = 40, color = &quot;black&quot;, fill = &quot;darkseagreen3&quot;) + labs(title = &quot;Distribution of Post-Graduation Unemployment rates at R2s&quot;, subtitle = &quot;&quot;, x = &quot;Unemployment Rate&quot;, y = &quot;Frequency&quot;) unemp_hist2 5.4 Wilcox signed-rank test Preliminary normality analysis of the median graduate debt distribution revealed levels of skewness that forbade the use of parametric comparison tests. A Wilcoxon signed-rank test was conducted in order to investigate the differences in median graduate debt between R1 and R2 institutions. Results indicate that median graduate debt significantly differed between R1 and R2 institutions (W = 982, p &lt; 0.01). While a difference in student debt was expected due to the aforementioned price of R1 institution tuition, descriptive statistics revealed graduates of R1 institutions graduated with less average debt (M = 19,652, SD = 3,750) than students from R2 institutions (M = 22,719, SD = 3,785). wilcox.test(formula=finR$grad_debt_mdn~finR$r_status) ## ## Wilcoxon rank sum test with continuity correction ## ## data: finR$grad_debt_mdn by finR$r_status ## W = 4430.5, p-value = 2.092e-11 ## alternative hypothesis: true location shift is not equal to 0 5.5 Welchs two-sample t-test In order to further explore student outcomes, the investigators turned their attention to the graduate unemployment rate. The distribution for unemployment exhibited skewness and kurtosis values within acceptable bounds so a Welchs two-sample t-test was conducted in order to find whether there were significant differences in unemployment rates between R1 and R2 graduates. The test revealed significant differences in rates of unemployment between R1 and R2 graduates t(118.36) = -2.6055, p = 0.01, 95%C.I. [-0.44, -0.06]. On average, the unemployment rate for R1 graduates (M = 3.293, SD = 0.51) was lower than that observed for R2 graduates (M = 3.545, SD = 0.83) t.test(finR$unemp_rate~finR$r_status) ## ## Welch Two Sample t-test ## ## data: finR$unemp_rate by finR$r_status ## t = -2.7918, df = 208.77, p-value = 0.005728 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.41446370 -0.07138832 ## sample estimates: ## mean in group 1 mean in group 2 ## 3.292901 3.535827 5.6 Regression model The racial diversity proportion of an institution was examined as a potential moderator of the relationship between institution type (R1 vs. R2) and unemployment rate. In the first step of the regression analysis institution type was entered into the model and accounted for 5% of the variance in unemployment rate, which was significant, R2= 0.52, F(1,122) = 6.82, p = 0.01. In the second step of the regression analysis the interaction effect between institution type and racial diversity proportion were entered into the model and explained an additional 4% of the variance in unemployment rate, which was significant, R2= 0.04, F(1,121) = 8.57, p &lt; 0.05. Racial diversity proportion was entered into the model last and accounted for 24% of the variance in unemployment rate, which was significant, R2= 0.24, F(1,122) = 38.8 , p &lt; 0.01. Thus, racial diversity proportion was a significant moderator of the relationship between institution type and unemployment rate. 5.7 Model step 1 predictor &lt;- finR$r_status outcome &lt;- finR$unemp_rate simplelm &lt;- lm(formula = outcome ~ predictor) 5.8 Model step 2 predictor &lt;- demR$white outcome &lt;- finR$unemp_rate moderatorlm &lt;- lm(formula = outcome ~ predictor) 5.9 Model step 3 predictor &lt;- as.integer(finR$r_status) moderator &lt;- demR$white interaction &lt;- predictor * moderator outcome &lt;- finR$unemp_rate interactionlm &lt;- lm(formula = outcome ~ interaction) 5.10 Complete model predictor &lt;- as.integer(finR$r_status) moderator &lt;- demR$white interaction &lt;- predictor * moderator outcome &lt;- finR$unemp_rate wlm &lt;- lm(formula = outcome ~ predictor + moderator + interaction) "],["discussion.html", "Section 6 Discussion 6.1 Interactive Chloropleths 6.2 Distribution", " Section 6 Discussion 6.1 Interactive Chloropleths library(plotly) library(tidyverse) library(purrr) library(RColorBrewer) library(tmap) g &lt;- list( scope = &#39;usa&#39;, projection = list(type = &#39;albers usa&#39;), lakecolor = toRGB(&#39;white&#39;) ) avgDM &lt;- distinct(debtMedian) plot_geo() %&gt;% add_trace( z = ~avgDM$avgDebt, span = I(0), colorscale = &#39;viridis&#39;, locations = avgDM$state, locationmode = &#39;USA-states&#39; ) %&gt;% colorbar(title = NULL) %&gt;% layout(geo = g, title = &quot;Avg. Faculty Salary by State (in USD)&quot;) r1_state &lt;- cdem %&gt;% filter(r_status == &quot;R1&quot;) %&gt;% group_by(state) %&gt;% distinct(university) %&gt;% count() r2_state &lt;- cdem %&gt;% filter(r_status == &quot;R2&quot;) %&gt;% group_by(state) %&gt;% distinct(university) %&gt;% count() g &lt;- list( scope = &#39;usa&#39;, projection = list(type = &#39;albers usa&#39;), lakecolor = toRGB(&#39;white&#39;) ) r1_map &lt;- plot_geo() %&gt;% add_trace( z = ~r1_state$n, span = I(1), colorscale = &#39;Portland&#39;, zauto = F, zmax = 10, zmin = 1, locations = r1_state$state, locationmode = &#39;USA-states&#39; ) %&gt;% colorbar(title = &quot;# of Schools&quot;) %&gt;% layout(geo = g, title = &quot;Number of R1 Schools per State&quot;) r2_map &lt;- plot_geo() %&gt;% add_trace( z = ~r2_state$n, span = I(1), colorscale = &#39;Portland&#39;, zauto = F, zmax = 10, zmin = 1, locations = r2_state$state, locationmode = &#39;USA-states&#39; ) %&gt;% colorbar(title = &quot;# of Schools&quot;) %&gt;% layout(geo = g, title = &quot;Number of R2 Schools per State&quot;) r1_map r2_map 6.2 Distribution dem_pal &lt;- c(&quot;darkolivegreen&quot;, &quot;darkolivegreen3&quot;, &quot;dodgerblue4&quot;, &quot;deepskyblue&quot;, &quot;lavenderblush4&quot;, &quot;lavenderblush2&quot;, &quot;palevioletred1&quot;, &quot;rosybrown1&quot;, &quot;tomato2&quot;, &quot;sienna 1&quot;, &quot;slateblue3&quot;, &quot;thistle1&quot;, &quot;orange&quot;, &quot;navajowhite2&quot;, &quot;lightskyblue3&quot;, &quot;lightsteelblue1&quot;) race_demographics &lt;- cdem %&gt;% select(university, r_status, white, black, hispanic, asian, indigenous, nhpi, nra, unknown) %&gt;% gather(&quot;race&quot;, &quot;percentage&quot;, 3:10, -university) race_demographics %&gt;% ggplot(aes(x = factor(r_status), y = percentage, fill = interaction(r_status, race))) + geom_violin() + stat_summary(fun=mean, geom=&quot;crossbar&quot;, linetype = 1, size=.2, color = &quot;red&quot;) + stat_summary(fun=median, geom=&quot;crossbar&quot;, linetype = 1, size=.2, color = &quot;black&quot;) + labs(title = &quot;Racial Demographics by Carnegie Classification&quot;, subtitle = &quot;Red line = mean | Black line = median\\n&quot;, x = &quot; &quot;, y = &quot;% of Student Body&quot;) + facet_wrap(~as.factor(race), nrow = 5) + scale_fill_manual(values = dem_pal) + theme_gray() + theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)), plot.title = element_text(hjust = .5), legend.position = &quot;none&quot;) #Demographics for sparse columns other_demographics &lt;- cdem %&gt;% select(university, r_status, female, first_gen, veteran) %&gt;% gather(&quot;demographic&quot;, &quot;percentage&quot;, 3:5, -university) other_demographics %&gt;% ggplot(aes(x = factor(r_status), y = percentage, fill = interaction(r_status, demographic))) + geom_violin() + stat_summary(fun=mean, geom=&quot;crossbar&quot;, linetype = 2, size=.1, color = &quot;red&quot;) + labs(title = &quot;Other Demographic Factors by Carnegie Classification&quot;, subtitle = &quot;&quot;, x = &quot; &quot;, y = &quot;% of Student Body&quot;) + facet_wrap(~as.factor(demographic), nrow = 5) + scale_fill_brewer(&quot;Paired&quot;) + theme_gray() + theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0)), plot.title = element_text(hjust = .5), legend.position = &quot;none&quot;) #FUN FACTS #Universities with the highest female/male ratio, the top 10 are mostly R2&#39;s for both categories cdem %&gt;% arrange(desc(female)) %&gt;% filter(female &gt; .50) %&gt;% distinct(university, .keep_all = TRUE) %&gt;% head(10) ## control university r_status state female ## 1 2 Thomas Jefferson University R2 PA 0.7743842 ## 2 1 CUNY Graduate School and University Center R1 NY 0.7729469 ## 3 2 Clark Atlanta University R2 GA 0.7631579 ## 4 2 The New School R2 NY 0.7536302 ## 5 2 Nova Southeastern University R2 FL 0.7459016 ## 6 2 University of New England R2 ME 0.7390681 ## 7 2 Azusa Pacific University R2 CA 0.7258883 ## 8 2 Howard University R2 DC 0.7177579 ## 9 2 Hampton University R2 VA 0.6915352 ## 10 2 Loyola University Chicago R2 IL 0.6876623 ## first_gen poverty_rate veteran unemp_rate white black hispanic asian ## 1 0.3293737 8.770079 0.004961565 3.535827 0.7195 0.0705 0.0136 0.0935 ## 2 0.4395604 14.690000 0.015700483 4.900000 0.2200 0.2893 0.3135 0.1183 ## 3 0.3158522 8.770079 0.004961565 3.535827 0.0004 0.8366 0.0040 0.0018 ## 4 0.1944012 8.770079 0.004961565 3.535827 0.3297 0.0548 0.1195 0.0926 ## 5 0.3733401 8.770079 0.007741348 3.535827 0.3296 0.1617 0.2988 0.0972 ## 6 0.2207308 8.770079 0.004961565 3.535827 0.8274 0.0127 0.0013 0.0354 ## 7 0.3798260 7.950000 0.005801305 3.830000 0.3726 0.0611 0.3322 0.0950 ## 8 0.2363936 10.550000 0.004961565 4.430000 0.0222 0.8863 0.0092 0.0141 ## 9 0.1975117 9.570000 0.004961565 4.010000 0.0142 0.9593 0.0131 0.0014 ## 10 0.2651357 8.770079 0.004545454 3.535827 0.5814 0.0444 0.1425 0.1156 ## indigenous nhpi nra unknown year ## 1 0.0000 0.0000 0.0081 0.0664 2015 ## 2 0.0024 0.0048 0.0228 0.0000 latest ## 3 0.0011 0.0000 0.0241 0.1321 2015 ## 4 0.0011 0.0016 0.3234 0.0400 2016 ## 5 0.0026 0.0009 0.0572 0.0319 2016 ## 6 0.0042 0.0000 0.0034 0.1021 2016 ## 7 0.0029 0.0106 0.0310 0.0233 latest ## 8 0.0013 0.0052 0.0616 0.0000 latest ## 9 0.0025 0.0003 0.0082 0.0008 latest ## 10 0.0006 0.0028 0.0490 0.0139 2015 cdem %&gt;% arrange(female) %&gt;% filter(female &lt; .50) %&gt;% distinct(university, .keep_all = TRUE) %&gt;% head(10) ## control university r_status state ## 1 1 New Jersey Institute of Technology R1 NJ ## 2 1 Missouri University of Science and Technology R2 MO ## 3 1 Colorado School of Mines R2 CO ## 4 1 Michigan Technological University R2 MI ## 5 2 Illinois Institute of Technology R2 IL ## 6 2 Stevens Institute of Technology R2 NJ ## 7 2 Clarkson University R2 NY ## 8 2 Rensselaer Polytechnic Institute R1 NY ## 9 2 Rochester Institute of Technology R2 NY ## 10 2 Worcester Polytechnic Institute R2 MA ## female first_gen poverty_rate veteran unemp_rate white black ## 1 0.1898865 0.3375479 7.624656 0.003439973 3.292901 0.3348 0.0857 ## 2 0.2278168 0.2257366 8.770079 0.004961565 3.535827 0.7815 0.0351 ## 3 0.2631579 0.1719745 8.770079 0.004961565 3.535827 0.7471 0.0104 ## 4 0.2765753 0.1732010 8.770079 0.004961565 3.535827 0.8609 0.0108 ## 5 0.2832370 0.3137255 8.770079 0.004961565 3.535827 0.3257 0.0585 ## 6 0.2847358 0.1549439 5.680000 0.004961565 3.140000 0.6433 0.0219 ## 7 0.2998555 0.1528710 7.160000 0.004961565 3.590000 0.8113 0.0245 ## 8 0.3294118 0.1306505 7.624656 0.003724733 3.292901 0.5907 0.0308 ## 9 0.3355602 0.2022427 8.770079 0.004961565 3.535827 0.6580 0.0495 ## 10 0.3477952 0.1429619 8.770079 0.004961565 3.535827 0.6294 0.0232 ## hispanic asian indigenous nhpi nra unknown year ## 1 0.2210 0.2214 0.0006 0.0006 0.0443 0.0604 2015 ## 2 0.0311 0.0299 0.0037 0.0007 0.0566 0.0348 2015 ## 3 0.0693 0.0488 0.0013 0.0007 0.0589 0.0088 2015 ## 4 0.0185 0.0105 0.0037 0.0005 0.0421 0.0254 2015 ## 5 0.1553 0.1300 0.0034 0.0014 0.2648 0.0428 2015 ## 6 0.1137 0.1459 0.0009 0.0000 0.0351 0.0392 latest ## 7 0.0473 0.0369 0.0030 0.0000 0.0228 0.0188 latest ## 8 0.0813 0.1012 0.0014 0.0002 0.1062 0.0187 2015 ## 9 0.0711 0.0756 0.0018 0.0002 0.0585 0.0542 2015 ## 10 0.0843 0.0457 0.0024 0.0000 0.1226 0.0625 2015 R1s &lt;- cdem %&gt;% filter(r_status == &quot;R1&quot;) ggplot(cdem, aes(x = female)) + geom_histogram(bins = 40, color = &quot;black&quot;, fill = &quot;pink&quot;) + labs(title = &quot;Distribution of Female/Male Gender ratio at R1s&quot;) R2s &lt;- cdem %&gt;% filter(r_status == &quot;R2&quot;) ggplot(R2s, aes(x = female)) + geom_histogram(bins = 40, color = &quot;black&quot;, fill = &quot;skyblue&quot;) + labs(title = &quot;Distribution of Female/Male Gender ratio at R2s&quot;) "],["references.html", "References", " References "]]
